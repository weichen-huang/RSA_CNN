{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# DNN class\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from scipy.stats import pearsonr\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import models as tv_models\n",
    "from dnnbrain_lib import ImageSet, VideoSet, dnn_mask, array_statistic, Stimulus, Activation\n",
    "class DNN:\n",
    "    \"\"\"\n",
    "    Deep neural network\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model : nn.Modules\n",
    "        DNN model\n",
    "    layer2loc : dict\n",
    "        Map layer name to its location in the DNN model\n",
    "    img_size : tuple\n",
    "        The input image size\n",
    "    train_transform : torchvision.transform\n",
    "        The transform used in training state\n",
    "    test_transform : torchvision.transform\n",
    "        The transform used in testing state\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = None\n",
    "        self.layer2loc = None\n",
    "        self.img_size = None  # (height, width)\n",
    "        self.train_transform = None\n",
    "        self.test_transform = None\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        raise NotImplementedError('This method should be implemented in subclasses.')\n",
    "\n",
    "    def save(self, fname):\n",
    "        \"\"\"\n",
    "        Save DNN parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : str\n",
    "            Output file name with suffix as .pth\n",
    "        \"\"\"\n",
    "        assert fname.endswith('.pth'), 'File suffix must be .pth'\n",
    "        torch.save(self.model.state_dict(), fname)\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Turn to evaluation mode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DNN\n",
    "            The evaluation mode of DNN\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def layer2module(self, layer):\n",
    "        \"\"\"\n",
    "        Get a PyTorch Module object according to the layer name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : str\n",
    "            Layer name\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        module : Module\n",
    "            PyTorch Module object\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This method should be implemented in subclasses.')\n",
    "\n",
    "    def compute_activation(self, stimuli, dmask, pool_method=None, cuda=False):\n",
    "        \"\"\"\n",
    "        Extract DNN activation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stimuli : Stimulus, ndarray\n",
    "            Input stimuli.           \n",
    "            If is Stimulus, loaded from files on the disk.\n",
    "            If is ndarray, its shape is (n_stim, n_chn, height, width)\n",
    "        dmask : Mask\n",
    "            The mask includes layers/channels/rows/columns of interest.\n",
    "        pool_method : str\n",
    "            pooling method, choices=(max, mean, median, L1, L2)\n",
    "        cuda : bool\n",
    "            use GPU or not\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        activation : Activation\n",
    "            DNN activation\n",
    "        \"\"\"\n",
    "        # prepare stimuli loader\n",
    "        if isinstance(stimuli, np.ndarray):\n",
    "            stim_set = []\n",
    "            for arr in stimuli:\n",
    "                img = Image.fromarray(arr.transpose((1, 2, 0)))\n",
    "                stim_set.append((self.test_transform(img), 0))\n",
    "        elif isinstance(stimuli, Stimulus):\n",
    "            if stimuli.header['type'] == 'image':\n",
    "                stim_set = ImageSet(stimuli.header['path'], stimuli.get('stimID'),\n",
    "                                    transform=self.test_transform)\n",
    "            elif stimuli.header['type'] == 'video':\n",
    "                stim_set = VideoSet(stimuli.header['path'], stimuli.get('stimID'),\n",
    "                                    transform=self.test_transform)\n",
    "            else:\n",
    "                raise TypeError('{} is not a supported stimulus type.'.format(stimuli.header['type']))\n",
    "        else:\n",
    "            raise TypeError('The input stimuli must be an instance of ndarray or Stimulus!')\n",
    "        data_loader = DataLoader(stim_set, 8, shuffle=False)\n",
    "\n",
    "        # -extract activation-\n",
    "        # prepare model\n",
    "        self.model.eval()\n",
    "        if cuda:\n",
    "            assert torch.cuda.is_available(), 'There is no CUDA available.'\n",
    "            self.model.to(torch.device('cuda'))\n",
    "\n",
    "        n_stim = len(stim_set)\n",
    "        activation = Activation()\n",
    "        for layer in dmask.layers:\n",
    "            # prepare dnn activation hook\n",
    "            acts_holder = []\n",
    "\n",
    "            def hook_act(module, input, output):\n",
    "\n",
    "                # copy activation\n",
    "                if cuda:\n",
    "                    acts = output.cpu().data.numpy().copy()\n",
    "                else:\n",
    "                    acts = output.detach().numpy().copy()\n",
    "\n",
    "                # unify dimension number\n",
    "                if acts.ndim == 4:\n",
    "                    pass\n",
    "                elif acts.ndim == 2:\n",
    "                    acts = acts[:, :, None, None]\n",
    "                else:\n",
    "                    raise ValueError('Unexpected activation shape:', acts.shape)\n",
    "\n",
    "                # mask activation\n",
    "                mask = dmask.get(layer)\n",
    "                acts = dnn_mask(acts, mask.get('chn'),\n",
    "                                mask.get('row'), mask.get('col'))\n",
    "\n",
    "                # pool activation\n",
    "                if pool_method is not None:\n",
    "                    acts = array_statistic(acts, pool_method, (2, 3), True)\n",
    "\n",
    "                # hold activation\n",
    "                acts_holder.extend(acts)\n",
    "\n",
    "            module = self.layer2module(layer)\n",
    "            hook_handle = module.register_forward_hook(hook_act)\n",
    "\n",
    "            # extract DNN activation\n",
    "            for stims, _ in data_loader:\n",
    "                # stimuli with shape as (n_stim, n_chn, height, width)\n",
    "                if cuda:\n",
    "                    stims = stims.to(torch.device('cuda'))\n",
    "                self.model(stims)\n",
    "                print('Extracted activation of {0}: {1}/{2}'.format(\n",
    "                    layer, len(acts_holder), n_stim))\n",
    "            activation.set(layer, np.asarray(acts_holder))\n",
    "\n",
    "            hook_handle.remove()\n",
    "\n",
    "        return activation\n",
    "\n",
    "    def get_kernel(self, layer, kernels=None):\n",
    "        \"\"\"\n",
    "        Get kernels' weights of the layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : str\n",
    "            Layer name\n",
    "        kernels : int, list\n",
    "            Serial numbers of kernels.\n",
    "            Start from 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : tensor\n",
    "            Kernel weights\n",
    "        \"\"\"\n",
    "        # get the module\n",
    "        module = self.layer2module(layer)\n",
    "\n",
    "        # get the weights\n",
    "        weights = module.weight\n",
    "        if kernels is not None:\n",
    "            # deal with kernel numbers\n",
    "            kernels = np.asarray(kernels)\n",
    "            assert np.all(kernels > 0), 'The kernel number should start from 1.'\n",
    "            kernels = kernels - 1\n",
    "            # get part of weights\n",
    "            weights = weights[kernels]\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def ablate(self, layer, channels=None):\n",
    "        \"\"\"\n",
    "        Ablate DNN kernels' weights\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : str\n",
    "            Layer name\n",
    "        channels : list\n",
    "            Sequence numbers of channels of interest.\n",
    "            If None, ablate the whole layer.\n",
    "        \"\"\"\n",
    "        # localize the module\n",
    "        module = self.layer2module(layer)\n",
    "\n",
    "        # ablate kernels' weights\n",
    "        if channels is None:\n",
    "            module.weight.data[:] = 0\n",
    "        else:\n",
    "            channels = [chn - 1 for chn in channels]\n",
    "            module.weight.data[channels] = 0\n",
    "\n",
    "    def train(self, data, n_epoch, task, optimizer=None, method='tradition', target=None,\n",
    "              data_train=False, data_validation=None):\n",
    "        \"\"\"\n",
    "        Train the DNN model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Stimulus, ndarray\n",
    "            Training data\n",
    "            \n",
    "            If is Stimulus, load stimuli from files on the disk.\n",
    "            Note, the data of the 'label' item in the Stimulus object will be used as\n",
    "            truth of the output when 'target' is None.\n",
    "            \n",
    "            If is ndarray, it contains stimuli with shape as (n_stim, n_chn, height, width).\n",
    "            Note, the truth data must be specified by 'target' parameter.\n",
    "        n_epoch : int\n",
    "            the number of epochs\n",
    "        task : str\n",
    "            Task function.\n",
    "            Choices=('classification', 'regression').\n",
    "        optimizer : object\n",
    "            Optimizer function.\n",
    "            \n",
    "            If is None, use Adam to optimize all parameters in dnn.\n",
    "            If is not None, it must be torch optimizer object.\n",
    "        method : str\n",
    "            Training method, by default is 'tradition'.\n",
    "            For some specific models (e.g. inception), loss needs to be calculated in another way.\n",
    "        target : ndarray\n",
    "            The output of the model.\n",
    "            Its shape is (n_stim,) for classification or (n_stim, n_feat) for regression.\n",
    "            Note: n_feat is the number of features of the last layer.\n",
    "        data_train : bool\n",
    "            If true, test model performance on the training data.\n",
    "        data_validation : Stimulus, ndarray\n",
    "            Validation data.\n",
    "            If is not None, test model performance on the validation data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_dict : dict\n",
    "            A dict containing the training score and loss information.\n",
    "                \n",
    "            +------------------+-------------+----------------------------------------------------------------+\n",
    "            |   Key            | Value type  |       Value description                                        |\n",
    "            +==================+=============+================================================================+\n",
    "            |  epoch_loss      |    list     | Losses of epochs.                                              |\n",
    "            +------------------+-------------+----------------------------------------------------------------+\n",
    "            |  step_loss       |    list     | Step losses of epochs.The indices are one-to-one               |\n",
    "            |                  |             | corresponding with the epoch_losses. Each element is a         | \n",
    "            |                  |             | list where elements are step losses of the corresponding epoch.| \n",
    "            +------------------+-------------+----------------------------------------------------------------+\n",
    "            | score_train      |    list     | Scores of epochs on training data.                             |\n",
    "            +------------------+-------------+----------------------------------------------------------------+\n",
    "            | score_validation |    list     | Scores of epochs on validation data.                           |\n",
    "            +------------------+-------------+----------------------------------------------------------------+\n",
    "        \"\"\"\n",
    "        # prepare data loader\n",
    "        if isinstance(data, np.ndarray):\n",
    "            stim_set = [Image.fromarray(arr.transpose((1, 2, 0))) for arr in data]\n",
    "            stim_set = [(self.train_transform(img), trg) for img, trg in zip(stim_set, target)]\n",
    "        elif isinstance(data, Stimulus):\n",
    "            if data.header['type'] == 'image':\n",
    "                stim_set = ImageSet(data.header['path'], data.get('stimID'),\n",
    "                                    data.get('label'), transform=self.train_transform)\n",
    "            elif data.header['type'] == 'video':\n",
    "                stim_set = VideoSet(data.header['path'], data.get('stimID'),\n",
    "                                    data.get('label'), transform=self.train_transform)\n",
    "            else:\n",
    "                raise TypeError(f\"{data.header['type']} is not a supported stimulus type.\")\n",
    "\n",
    "            if target is not None:\n",
    "                # We presume small quantity stimuli will be used in this way.\n",
    "                # Usually hundreds or thousands such as fMRI stimuli.\n",
    "                stim_set = [(img, trg) for img, trg in zip(stim_set[:][0], target)]\n",
    "        else:\n",
    "            raise TypeError('The input data must be an instance of ndarray or Stimulus!')\n",
    "        data_loader = DataLoader(stim_set, 64, shuffle=True)\n",
    "\n",
    "        # prepare criterion\n",
    "        if task == 'classification':\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif task == 'regression':\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported task: {task}')\n",
    "\n",
    "        # prepare optimizer\n",
    "        if optimizer is None:\n",
    "            optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "        # start train\n",
    "        train_dict = dict()\n",
    "        train_dict['step_loss'] = []\n",
    "        train_dict['epoch_loss'] = []\n",
    "        train_dict['score_train'] = []\n",
    "        train_dict['score_validation'] = []\n",
    "        time1 = time.time()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.train()\n",
    "        self.model.to(device)\n",
    "        for epoch in range(n_epoch):\n",
    "            print(f'Epoch-{epoch+1}/{n_epoch}')\n",
    "            print('-' * 10)\n",
    "            time2 = time.time()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            step_losses = []\n",
    "            for inputs, targets in data_loader:\n",
    "                inputs.requires_grad_(True)\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    if method == 'tradition':\n",
    "                        outputs = self.model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                    elif method == 'inception':\n",
    "                        # Google inception model\n",
    "                        outputs, aux_outputs = self.model(inputs)\n",
    "                        loss1 = criterion(outputs, targets)\n",
    "                        loss2 = criterion(aux_outputs, targets)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        raise Exception(f'not supported method-{method}')\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Statistics loss in every batch\n",
    "                step_losses.append(loss.item())\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # calculate loss\n",
    "            train_dict['step_loss'].append(step_losses)\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            print(f'Loss: {epoch_loss}')\n",
    "            train_dict['epoch_loss'].append(epoch_loss)\n",
    "\n",
    "            # test performance\n",
    "            if data_train:\n",
    "                test_dict = self.test(data, task, target, torch.cuda.is_available())\n",
    "                print(f\"Score_on_train: {test_dict['score']}\")\n",
    "                train_dict['score_train'].append(test_dict['score'])\n",
    "                self.model.train()\n",
    "            if data_validation is not None:\n",
    "                test_dict = self.test(data_validation, task, target, torch.cuda.is_available())\n",
    "                print(f\"Score_on_test: {test_dict['score']}\")\n",
    "                train_dict['score_validation'].append(test_dict['score'])\n",
    "                self.model.train()\n",
    "\n",
    "            # print time of a epoch\n",
    "            epoch_time = time.time() - time2\n",
    "            print('This epoch costs {:.0f}m {:.0f}s\\n'.format(epoch_time // 60, epoch_time % 60))\n",
    "\n",
    "        time_elapsed = time.time() - time1\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "        # back to cpu\n",
    "        self.model.to(torch.device('cpu'))\n",
    "        return train_dict\n",
    "\n",
    "    def test(self, data, task, target=None, cuda=False):\n",
    "        \"\"\"\n",
    "        Test the DNN model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Stimulus, ndarray\n",
    "            Testing data.\n",
    "            \n",
    "            If is Stimulus, load stimuli from files on the disk.\n",
    "            Note, the data of the 'label' item in the Stimulus object will be used as\n",
    "            output of the model when 'target' is None.\n",
    "            \n",
    "            If is ndarray, it contains stimuli with shape as (n_stim, n_chn, height, width).\n",
    "            Note, the output data must be specified by 'target' parameter.\n",
    "        task : str\n",
    "            Choices=(classification, regression)\n",
    "        target : ndarray\n",
    "            The output of the model.\n",
    "            Its shape is (n_stim,) for classification or (n_stim, n_feat) for regression.\n",
    "            Note, n_feat is the number of features of the last layer.\n",
    "        cuda : bool\n",
    "            Use GPU or not\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        test_dict : dict\n",
    "            A dict containing the test score information.\n",
    "                             \n",
    "            +----------------+-------------+---------------+------------------------------------------+\n",
    "            |       Task     |   Key       |  Value type   |       Value description                  |\n",
    "            +================+=============+===============+==========================================+\n",
    "            | classification |  pred_value |     array     | Prediction labels by the model.          | \n",
    "            |                |             |               | 2d array with shape as (n_stim, n_class).|\n",
    "            |                |             |               | Each row's labels are sorted from        |\n",
    "            |                |             |               | large to small their probabilities.      |\n",
    "            |                |             |               | score for each channel.                  |\n",
    "            |                +-------------+---------------+------------------------------------------+\n",
    "            |                |  true_value |     array     | Observation labels.                      |\n",
    "            |                +-------------+---------------+------------------------------------------+\n",
    "            |                |    score    |     float     | Prediction accuracy.                     |\n",
    "            +----------------+-------------+---------------+------------------------------------------+\n",
    "            |  regression    |  pred_value |     array     | Prediction labels by the model.          | \n",
    "            |                +-------------+---------------+------------------------------------------+\n",
    "            |                |  true_value |     array     | Observation labels.                      |\n",
    "            |                +-------------+---------------+------------------------------------------+\n",
    "            |                |    score    |     float     | Prediction accuracy.                     |\n",
    "            +----------------+-------------+---------------+------------------------------------------+\n",
    "        \"\"\"\n",
    "        # prepare data loader\n",
    "        if isinstance(data, np.ndarray):\n",
    "            stim_set = [Image.fromarray(arr.transpose((1, 2, 0))) for arr in data]\n",
    "            stim_set = [(self.test_transform(img), trg) for img, trg in zip(stim_set, target)]\n",
    "        elif isinstance(data, Stimulus):\n",
    "            if data.header['type'] == 'image':\n",
    "                stim_set = ImageSet(data.header['path'], data.get('stimID'),\n",
    "                                    data.get('label'), transform=self.test_transform)\n",
    "            elif data.header['type'] == 'video':\n",
    "                stim_set = VideoSet(data.header['path'], data.get('stimID'),\n",
    "                                    data.get('label'), transform=self.test_transform)\n",
    "            else:\n",
    "                raise TypeError(f\"{data.header['type']} is not a supported stimulus type.\")\n",
    "\n",
    "            if target is not None:\n",
    "                # We presume small quantity stimuli will be used in this way.\n",
    "                # Usually hundreds or thousands such as fMRI stimuli.\n",
    "                stim_set = [(img, trg) for img, trg in zip(stim_set[:][0], target)]\n",
    "        else:\n",
    "            raise TypeError('The input data must be an instance of ndarray or Stimulus!')\n",
    "        data_loader = DataLoader(stim_set, 8, shuffle=False)\n",
    "\n",
    "        # start test\n",
    "        self.model.eval()\n",
    "        if cuda:\n",
    "            assert torch.cuda.is_available(), 'There is no CUDA available.'\n",
    "            self.model.to(torch.device('cuda'))\n",
    "        pred_values = []\n",
    "        true_values = []\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, targets) in enumerate(data_loader):\n",
    "                if cuda:\n",
    "                    inputs = inputs.to(torch.device('cuda'))\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                # collect outputs\n",
    "                if cuda:\n",
    "                    pred_values.extend(outputs.cpu().detach().numpy())\n",
    "                else:\n",
    "                    pred_values.extend(outputs.detach().numpy())\n",
    "                true_values.extend(targets.numpy())\n",
    "        pred_values = np.array(pred_values)\n",
    "        true_values = np.array(true_values)\n",
    "\n",
    "        # prepare output info\n",
    "        test_dict = dict()\n",
    "        if task == 'classification':\n",
    "            pred_values = np.argsort(-pred_values, 1)\n",
    "            # calculate accuracy\n",
    "            score = np.mean(pred_values[:, 0] == true_values)\n",
    "        elif task == 'regression':\n",
    "            # calculate r_square\n",
    "            r, _ = pearsonr(pred_values.ravel(), true_values.ravel())\n",
    "            score = r ** 2\n",
    "        else:\n",
    "            raise ValueError(f'Not supported task: {task}')\n",
    "        test_dict['pred_value'] = pred_values\n",
    "        test_dict['true_value'] = true_values\n",
    "        test_dict['score'] = score\n",
    "\n",
    "        return test_dict\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed the model with the inputs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : Tensor\n",
    "            A tensor with shape as (n_stim, n_chn, n_height, n_width)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : Tensor\n",
    "            Output of the model, usually with shape as (n_stim, n_feat).\n",
    "            n_feat is the number of out features in the last layer of the model.\n",
    "        \"\"\"\n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:19:21.927331Z",
     "start_time": "2024-07-01T18:19:16.959167Z"
    }
   },
   "id": "581ab3d2ccb07a32"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# resnet\n",
    "\n",
    "class Resnet152(DNN):\n",
    "    def __init__(self):\n",
    "        super(Resnet152, self).__init__()\n",
    "        self.model = tv_models.resnet152()\n",
    "        print(self.model)\n",
    "        self.layer2loc = {'conv':                ('conv1',),\n",
    "                          'bn':                  ('bn1',),\n",
    "                          'relu':                ('relu',),\n",
    "                          'maxpool':             ('maxpool',),\n",
    "                          'layer1_bottleneck0':  ('layer1', '0'),\n",
    "                          'layer1_bottleneck1':  ('layer1', '1'),\n",
    "                          'layer1_bottleneck2':  ('layer1', '2'),\n",
    "                          'layer2_bottleneck0':  ('layer2', '0'),\n",
    "                          'layer2_bottleneck1':  ('layer2', '1'),\n",
    "                          'layer2_bottleneck2':  ('layer2', '2'),\n",
    "                          'layer2_bottleneck3':  ('layer2', '3'),\n",
    "                          'layer2_bottleneck4':  ('layer2', '4'),\n",
    "                          'layer2_bottleneck5':  ('layer2', '5'),\n",
    "                          'layer2_bottleneck6':  ('layer2', '6'),\n",
    "                          'layer2_bottleneck7':  ('layer2', '7'),\n",
    "                          'layer3_bottleneck0':  ('layer3', '0'),\n",
    "                          'layer3_bottleneck1':  ('layer3', '1'),\n",
    "                          'layer3_bottleneck2':  ('layer3', '2'),\n",
    "                          'layer3_bottleneck3':  ('layer3', '3'),\n",
    "                          'layer3_bottleneck4':  ('layer3', '4'),\n",
    "                          'layer3_bottleneck5':  ('layer3', '5'),\n",
    "                          'layer3_bottleneck6':  ('layer3', '6'),\n",
    "                          'layer3_bottleneck7':  ('layer3', '7'),\n",
    "                          'layer3_bottleneck8':  ('layer3', '8'),\n",
    "                          'layer3_bottleneck9':  ('layer3', '9'),\n",
    "                          'layer3_bottleneck10': ('layer3', '10'),\n",
    "                          'layer3_bottleneck11': ('layer3', '11'),\n",
    "                          'layer3_bottleneck12': ('layer3', '12'),\n",
    "                          'layer3_bottleneck13': ('layer3', '13'),\n",
    "                          'layer3_bottleneck14': ('layer3', '14'),\n",
    "                          'layer3_bottleneck15': ('layer3', '15'),\n",
    "                          'layer3_bottleneck16': ('layer3', '16'),\n",
    "                          'layer3_bottleneck17': ('layer3', '17'),\n",
    "                          'layer3_bottleneck18': ('layer3', '18'),\n",
    "                          'layer3_bottleneck19': ('layer3', '19'),\n",
    "                          'layer3_bottleneck20': ('layer3', '20'),\n",
    "                          'layer3_bottleneck21': ('layer3', '21'),\n",
    "                          'layer3_bottleneck22': ('layer3', '22'),\n",
    "                          'layer3_bottleneck23': ('layer3', '23'),\n",
    "                          'layer3_bottleneck24': ('layer3', '24'),\n",
    "                          'layer3_bottleneck25': ('layer3', '25'),\n",
    "                          'layer3_bottleneck26': ('layer3', '26'),\n",
    "                          'layer3_bottleneck27': ('layer3', '27'),\n",
    "                          'layer3_bottleneck28': ('layer3', '28'),\n",
    "                          'layer3_bottleneck29': ('layer3', '29'),\n",
    "                          'layer3_bottleneck30': ('layer3', '30'),\n",
    "                          'layer3_bottleneck31': ('layer3', '31'),\n",
    "                          'layer3_bottleneck32': ('layer3', '32'),\n",
    "                          'layer3_bottleneck33': ('layer3', '33'),\n",
    "                          'layer3_bottleneck34': ('layer3', '34'),\n",
    "                          'layer3_bottleneck35': ('layer3', '35'),\n",
    "                          'layer4_bottleneck0':  ('layer4', '0'),\n",
    "                          'layer4_bottleneck1':  ('layer4', '1'),\n",
    "                          'layer4_bottleneck2':  ('layer4', '2'),\n",
    "                          'avgpool':             ('avgpool',),\n",
    "                          'fc':                  ('fc',)}\n",
    "        self.img_size = (224, 224)\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(self.img_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return list(self.layer2loc.keys())\n",
    "\n",
    "    def layer2module(self, layer):\n",
    "        \"\"\"\n",
    "        Get a PyTorch Module object according to the layer name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : str\n",
    "            layer name\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        module : Module\n",
    "            PyTorch Module object\n",
    "        \"\"\"\n",
    "        module = self.model\n",
    "        for k in self.layer2loc[layer]:\n",
    "            module = module._modules[k]\n",
    "\n",
    "        return module\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:19:21.940604Z",
     "start_time": "2024-07-01T18:19:21.927730Z"
    }
   },
   "id": "95d119b4ea67a66e"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# resnet 18\n",
    "\n",
    "\"\"\"ResNet(\n",
    "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (relu): ReLU(inplace=True)\n",
    "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "  (layer1): Sequential(\n",
    "    (0): BasicBlock(\n",
    "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (1): BasicBlock(\n",
    "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "  )\n",
    "  (layer2): Sequential(\n",
    "    (0): BasicBlock(\n",
    "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (downsample): Sequential(\n",
    "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (1): BasicBlock(\n",
    "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "  )\n",
    "  (layer3): Sequential(\n",
    "    (0): BasicBlock(\n",
    "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (downsample): Sequential(\n",
    "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (1): BasicBlock(\n",
    "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "  )\n",
    "  (layer4): Sequential(\n",
    "    (0): BasicBlock(\n",
    "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (downsample): Sequential(\n",
    "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (1): BasicBlock(\n",
    "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "  )\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "# resnet\n",
    "\n",
    "class Resnet18(DNN):\n",
    "    def __init__(self, path=\"\"):\n",
    "        super(Resnet18, self).__init__()\n",
    "        self.model = tv_models.resnet18()\n",
    "        if path:\n",
    "            state_dict = torch.load(path, map_location=\"cpu\")\n",
    "            print(state_dict.keys)\n",
    "            \n",
    "            for key in list(state_dict.keys()):\n",
    "                if \"_orig_mod.fc.weight\" == key or \"_orig_mod.fc.bias\" == key:\n",
    "                    print(key)\n",
    "                    continue\n",
    "                state_dict[key.replace('_orig_mod.', '')] = state_dict.pop(key)\n",
    "\n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "        print(self.model)\n",
    "        self.layer2loc = {'conv':                ('conv1',),\n",
    "                          'bn':                  ('bn1',),\n",
    "                          'relu':                ('relu',),\n",
    "                          'maxpool':             ('maxpool',),\n",
    "                          'layer1_bottleneck0':  ('layer1', '0'),\n",
    "                          'layer1_bottleneck1':  ('layer1', '1'),\n",
    "                          'layer2_bottleneck0':  ('layer2', '0'),\n",
    "                          'layer2_bottleneck1':  ('layer2', '1'),\n",
    "                          'layer3_bottleneck0':  ('layer3', '0'),\n",
    "                          'layer3_bottleneck1':  ('layer3', '1'),\n",
    "                          'layer4_bottleneck0':  ('layer4', '0'),\n",
    "                          'layer4_bottleneck1':  ('layer4', '1'),\n",
    "                          'avgpool':             ('avgpool',),\n",
    "                          'fc':                  ('fc',)}\n",
    "        self.img_size = (224, 224)\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(self.img_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return list(self.layer2loc.keys())\n",
    "\n",
    "    def layer2module(self, layer):\n",
    "        \"\"\"\n",
    "        Get a PyTorch Module object according to the layer name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : str\n",
    "            layer name\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        module : Module\n",
    "            PyTorch Module object\n",
    "        \"\"\"\n",
    "        module = self.model\n",
    "        for k in self.layer2loc[layer]:\n",
    "            module = module._modules[k]\n",
    "\n",
    "        return module\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:32:52.042061Z",
     "start_time": "2024-07-01T18:32:52.025124Z"
    }
   },
   "id": "8fcedbf341d65b2b"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "class AlexNet(DNN):\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.model = tv_models.alexnet()\n",
    "\n",
    "        self.layer2loc = {'conv1':          ('features', '0'),\n",
    "                          'conv1_relu':     ('features', '1'),\n",
    "                          'conv1_maxpool':  ('features', '2'),\n",
    "                          'conv2':          ('features', '3'),\n",
    "                          'conv2_relu':     ('features', '4'),\n",
    "                          'conv2_maxpool':  ('features', '5'),\n",
    "                          'conv3':          ('features', '6'),\n",
    "                          'conv3_relu':     ('features', '7'),\n",
    "                          'conv4':          ('features', '8'),\n",
    "                          'conv4_relu':     ('features', '9'),\n",
    "                          'conv5':          ('features', '10'),\n",
    "                          'conv5_relu':     ('features', '11'),\n",
    "                          'conv5_maxpool':  ('features', '12'),\n",
    "                          'fc1':            ('classifier', '1'),\n",
    "                          'fc1_relu':       ('classifier', '2'),\n",
    "                          'fc2':            ('classifier', '4'),\n",
    "                          'fc2_relu':       ('classifier', '5'),\n",
    "                          'fc3':            ('classifier', '6')}\n",
    "        self.img_size = (224, 224)\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(self.img_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        \"\"\"\n",
    "        Get list of layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        layers : list\n",
    "            The list of layer name\n",
    "        \"\"\"\n",
    "        return list(self.layer2loc.keys())\n",
    "\n",
    "    def layer2module(self, layer):\n",
    "        \"\"\"\n",
    "        Get a PyTorch Module object according to the layer name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : str\n",
    "            Layer name\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        module : Module\n",
    "            PyTorch Module object\n",
    "        \"\"\"\n",
    "        module = self.model\n",
    "        for k in self.layer2loc[layer]:\n",
    "            module = module._modules[k]\n",
    "\n",
    "        return module\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:32:52.233061Z",
     "start_time": "2024-07-01T18:32:52.226105Z"
    }
   },
   "id": "235183b8132d7a9b"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "from dnnbrain_lib import gen_dmask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:32:52.409940Z",
     "start_time": "2024-07-01T18:32:52.407067Z"
    }
   },
   "id": "e23d2e4f73974935"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dnn = Resnet18()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:32:52.808562Z",
     "start_time": "2024-07-01T18:32:52.554802Z"
    }
   },
   "id": "b54ac78aa1312530"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# vars\n",
    "\n",
    "stim_path = \"data/all_5000scenes.stim.csv\"\n",
    "layer = [\"layer1_bottleneck1\", \"layer2_bottleneck1\", \"layer3_bottleneck1\", \"layer4_bottleneck1\"]\n",
    "out_path = \"data/alex.act.h5\"\n",
    "rdm_out = \"data/alex.rdm.h5\"\n",
    "meth = \"pca\"\n",
    "n_feats = 100\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:32:52.816009Z",
     "start_time": "2024-07-01T18:32:52.809442Z"
    }
   },
   "id": "c44ae6c72803672d"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method keys of collections.OrderedDict object at 0x331cf5140>\n",
      "_orig_mod.fc.weight\n",
      "_orig_mod.fc.bias\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vv/86qn_w6575919fvtrm5c3fmc0000gq/T/ipykernel_79030/385727301.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "dnn = Resnet18(\"data/model.pth\")\n",
    "stimuli = Stimulus()\n",
    "stimuli.load(stim_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:32:54.125617Z",
     "start_time": "2024-07-01T18:32:53.705130Z"
    }
   },
   "id": "e91278e328dd65fb"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "channels = 'all' \n",
    "dmask = gen_dmask(layer, channels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T18:32:58.440555Z",
     "start_time": "2024-07-01T18:32:58.434576Z"
    }
   },
   "id": "98248100184fd32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted activation of layer1_bottleneck1: 8/4916\n",
      "Extracted activation of layer1_bottleneck1: 16/4916\n",
      "Extracted activation of layer1_bottleneck1: 24/4916\n",
      "Extracted activation of layer1_bottleneck1: 32/4916\n",
      "Extracted activation of layer1_bottleneck1: 40/4916\n",
      "Extracted activation of layer1_bottleneck1: 48/4916\n",
      "Extracted activation of layer1_bottleneck1: 56/4916\n",
      "Extracted activation of layer1_bottleneck1: 64/4916\n",
      "Extracted activation of layer1_bottleneck1: 72/4916\n",
      "Extracted activation of layer1_bottleneck1: 80/4916\n",
      "Extracted activation of layer1_bottleneck1: 88/4916\n",
      "Extracted activation of layer1_bottleneck1: 96/4916\n",
      "Extracted activation of layer1_bottleneck1: 104/4916\n",
      "Extracted activation of layer1_bottleneck1: 112/4916\n",
      "Extracted activation of layer1_bottleneck1: 120/4916\n",
      "Extracted activation of layer1_bottleneck1: 128/4916\n",
      "Extracted activation of layer1_bottleneck1: 136/4916\n",
      "Extracted activation of layer1_bottleneck1: 144/4916\n",
      "Extracted activation of layer1_bottleneck1: 152/4916\n",
      "Extracted activation of layer1_bottleneck1: 160/4916\n",
      "Extracted activation of layer1_bottleneck1: 168/4916\n",
      "Extracted activation of layer1_bottleneck1: 176/4916\n",
      "Extracted activation of layer1_bottleneck1: 184/4916\n",
      "Extracted activation of layer1_bottleneck1: 192/4916\n",
      "Extracted activation of layer1_bottleneck1: 200/4916\n",
      "Extracted activation of layer1_bottleneck1: 208/4916\n",
      "Extracted activation of layer1_bottleneck1: 216/4916\n",
      "Extracted activation of layer1_bottleneck1: 224/4916\n",
      "Extracted activation of layer1_bottleneck1: 232/4916\n",
      "Extracted activation of layer1_bottleneck1: 240/4916\n",
      "Extracted activation of layer1_bottleneck1: 248/4916\n",
      "Extracted activation of layer1_bottleneck1: 256/4916\n",
      "Extracted activation of layer1_bottleneck1: 264/4916\n",
      "Extracted activation of layer1_bottleneck1: 272/4916\n",
      "Extracted activation of layer1_bottleneck1: 280/4916\n",
      "Extracted activation of layer1_bottleneck1: 288/4916\n",
      "Extracted activation of layer1_bottleneck1: 296/4916\n",
      "Extracted activation of layer1_bottleneck1: 304/4916\n",
      "Extracted activation of layer1_bottleneck1: 312/4916\n",
      "Extracted activation of layer1_bottleneck1: 320/4916\n",
      "Extracted activation of layer1_bottleneck1: 328/4916\n",
      "Extracted activation of layer1_bottleneck1: 336/4916\n",
      "Extracted activation of layer1_bottleneck1: 344/4916\n",
      "Extracted activation of layer1_bottleneck1: 352/4916\n",
      "Extracted activation of layer1_bottleneck1: 360/4916\n",
      "Extracted activation of layer1_bottleneck1: 368/4916\n",
      "Extracted activation of layer1_bottleneck1: 376/4916\n",
      "Extracted activation of layer1_bottleneck1: 384/4916\n",
      "Extracted activation of layer1_bottleneck1: 392/4916\n",
      "Extracted activation of layer1_bottleneck1: 400/4916\n",
      "Extracted activation of layer1_bottleneck1: 408/4916\n",
      "Extracted activation of layer1_bottleneck1: 416/4916\n",
      "Extracted activation of layer1_bottleneck1: 424/4916\n",
      "Extracted activation of layer1_bottleneck1: 432/4916\n",
      "Extracted activation of layer1_bottleneck1: 440/4916\n",
      "Extracted activation of layer1_bottleneck1: 448/4916\n",
      "Extracted activation of layer1_bottleneck1: 456/4916\n",
      "Extracted activation of layer1_bottleneck1: 464/4916\n",
      "Extracted activation of layer1_bottleneck1: 472/4916\n",
      "Extracted activation of layer1_bottleneck1: 480/4916\n",
      "Extracted activation of layer1_bottleneck1: 488/4916\n",
      "Extracted activation of layer1_bottleneck1: 496/4916\n",
      "Extracted activation of layer1_bottleneck1: 504/4916\n",
      "Extracted activation of layer1_bottleneck1: 512/4916\n",
      "Extracted activation of layer1_bottleneck1: 520/4916\n",
      "Extracted activation of layer1_bottleneck1: 528/4916\n",
      "Extracted activation of layer1_bottleneck1: 536/4916\n",
      "Extracted activation of layer1_bottleneck1: 544/4916\n",
      "Extracted activation of layer1_bottleneck1: 552/4916\n",
      "Extracted activation of layer1_bottleneck1: 560/4916\n",
      "Extracted activation of layer1_bottleneck1: 568/4916\n",
      "Extracted activation of layer1_bottleneck1: 576/4916\n",
      "Extracted activation of layer1_bottleneck1: 584/4916\n",
      "Extracted activation of layer1_bottleneck1: 592/4916\n",
      "Extracted activation of layer1_bottleneck1: 600/4916\n",
      "Extracted activation of layer1_bottleneck1: 608/4916\n",
      "Extracted activation of layer1_bottleneck1: 616/4916\n",
      "Extracted activation of layer1_bottleneck1: 624/4916\n",
      "Extracted activation of layer1_bottleneck1: 632/4916\n",
      "Extracted activation of layer1_bottleneck1: 640/4916\n",
      "Extracted activation of layer1_bottleneck1: 648/4916\n",
      "Extracted activation of layer1_bottleneck1: 656/4916\n",
      "Extracted activation of layer1_bottleneck1: 664/4916\n",
      "Extracted activation of layer1_bottleneck1: 672/4916\n",
      "Extracted activation of layer1_bottleneck1: 680/4916\n",
      "Extracted activation of layer1_bottleneck1: 688/4916\n",
      "Extracted activation of layer1_bottleneck1: 696/4916\n",
      "Extracted activation of layer1_bottleneck1: 704/4916\n",
      "Extracted activation of layer1_bottleneck1: 712/4916\n",
      "Extracted activation of layer1_bottleneck1: 720/4916\n",
      "Extracted activation of layer1_bottleneck1: 728/4916\n",
      "Extracted activation of layer1_bottleneck1: 736/4916\n",
      "Extracted activation of layer1_bottleneck1: 744/4916\n",
      "Extracted activation of layer1_bottleneck1: 752/4916\n",
      "Extracted activation of layer1_bottleneck1: 760/4916\n",
      "Extracted activation of layer1_bottleneck1: 768/4916\n",
      "Extracted activation of layer1_bottleneck1: 776/4916\n",
      "Extracted activation of layer1_bottleneck1: 784/4916\n",
      "Extracted activation of layer1_bottleneck1: 792/4916\n",
      "Extracted activation of layer1_bottleneck1: 800/4916\n",
      "Extracted activation of layer1_bottleneck1: 808/4916\n",
      "Extracted activation of layer1_bottleneck1: 816/4916\n",
      "Extracted activation of layer1_bottleneck1: 824/4916\n",
      "Extracted activation of layer1_bottleneck1: 832/4916\n",
      "Extracted activation of layer1_bottleneck1: 840/4916\n",
      "Extracted activation of layer1_bottleneck1: 848/4916\n",
      "Extracted activation of layer1_bottleneck1: 856/4916\n",
      "Extracted activation of layer1_bottleneck1: 864/4916\n",
      "Extracted activation of layer1_bottleneck1: 872/4916\n",
      "Extracted activation of layer1_bottleneck1: 880/4916\n",
      "Extracted activation of layer1_bottleneck1: 888/4916\n",
      "Extracted activation of layer1_bottleneck1: 896/4916\n",
      "Extracted activation of layer1_bottleneck1: 904/4916\n",
      "Extracted activation of layer1_bottleneck1: 912/4916\n",
      "Extracted activation of layer1_bottleneck1: 920/4916\n",
      "Extracted activation of layer1_bottleneck1: 928/4916\n",
      "Extracted activation of layer1_bottleneck1: 936/4916\n",
      "Extracted activation of layer1_bottleneck1: 944/4916\n",
      "Extracted activation of layer1_bottleneck1: 952/4916\n",
      "Extracted activation of layer1_bottleneck1: 960/4916\n",
      "Extracted activation of layer1_bottleneck1: 968/4916\n",
      "Extracted activation of layer1_bottleneck1: 976/4916\n",
      "Extracted activation of layer1_bottleneck1: 984/4916\n",
      "Extracted activation of layer1_bottleneck1: 992/4916\n",
      "Extracted activation of layer1_bottleneck1: 1000/4916\n",
      "Extracted activation of layer1_bottleneck1: 1008/4916\n",
      "Extracted activation of layer1_bottleneck1: 1016/4916\n",
      "Extracted activation of layer1_bottleneck1: 1024/4916\n",
      "Extracted activation of layer1_bottleneck1: 1032/4916\n",
      "Extracted activation of layer1_bottleneck1: 1040/4916\n",
      "Extracted activation of layer1_bottleneck1: 1048/4916\n",
      "Extracted activation of layer1_bottleneck1: 1056/4916\n",
      "Extracted activation of layer1_bottleneck1: 1064/4916\n",
      "Extracted activation of layer1_bottleneck1: 1072/4916\n",
      "Extracted activation of layer1_bottleneck1: 1080/4916\n",
      "Extracted activation of layer1_bottleneck1: 1088/4916\n",
      "Extracted activation of layer1_bottleneck1: 1096/4916\n",
      "Extracted activation of layer1_bottleneck1: 1104/4916\n",
      "Extracted activation of layer1_bottleneck1: 1112/4916\n",
      "Extracted activation of layer1_bottleneck1: 1120/4916\n",
      "Extracted activation of layer1_bottleneck1: 1128/4916\n",
      "Extracted activation of layer1_bottleneck1: 1136/4916\n",
      "Extracted activation of layer1_bottleneck1: 1144/4916\n",
      "Extracted activation of layer1_bottleneck1: 1152/4916\n",
      "Extracted activation of layer1_bottleneck1: 1160/4916\n",
      "Extracted activation of layer1_bottleneck1: 1168/4916\n",
      "Extracted activation of layer1_bottleneck1: 1176/4916\n",
      "Extracted activation of layer1_bottleneck1: 1184/4916\n",
      "Extracted activation of layer1_bottleneck1: 1192/4916\n",
      "Extracted activation of layer1_bottleneck1: 1200/4916\n",
      "Extracted activation of layer1_bottleneck1: 1208/4916\n",
      "Extracted activation of layer1_bottleneck1: 1216/4916\n",
      "Extracted activation of layer1_bottleneck1: 1224/4916\n",
      "Extracted activation of layer1_bottleneck1: 1232/4916\n",
      "Extracted activation of layer1_bottleneck1: 1240/4916\n",
      "Extracted activation of layer1_bottleneck1: 1248/4916\n",
      "Extracted activation of layer1_bottleneck1: 1256/4916\n",
      "Extracted activation of layer1_bottleneck1: 1264/4916\n",
      "Extracted activation of layer1_bottleneck1: 1272/4916\n",
      "Extracted activation of layer1_bottleneck1: 1280/4916\n",
      "Extracted activation of layer1_bottleneck1: 1288/4916\n",
      "Extracted activation of layer1_bottleneck1: 1296/4916\n",
      "Extracted activation of layer1_bottleneck1: 1304/4916\n",
      "Extracted activation of layer1_bottleneck1: 1312/4916\n",
      "Extracted activation of layer1_bottleneck1: 1320/4916\n",
      "Extracted activation of layer1_bottleneck1: 1328/4916\n",
      "Extracted activation of layer1_bottleneck1: 1336/4916\n",
      "Extracted activation of layer1_bottleneck1: 1344/4916\n",
      "Extracted activation of layer1_bottleneck1: 1352/4916\n",
      "Extracted activation of layer1_bottleneck1: 1360/4916\n",
      "Extracted activation of layer1_bottleneck1: 1368/4916\n",
      "Extracted activation of layer1_bottleneck1: 1376/4916\n",
      "Extracted activation of layer1_bottleneck1: 1384/4916\n",
      "Extracted activation of layer1_bottleneck1: 1392/4916\n",
      "Extracted activation of layer1_bottleneck1: 1400/4916\n",
      "Extracted activation of layer1_bottleneck1: 1408/4916\n",
      "Extracted activation of layer1_bottleneck1: 1416/4916\n",
      "Extracted activation of layer1_bottleneck1: 1424/4916\n",
      "Extracted activation of layer1_bottleneck1: 1432/4916\n",
      "Extracted activation of layer1_bottleneck1: 1440/4916\n",
      "Extracted activation of layer1_bottleneck1: 1448/4916\n",
      "Extracted activation of layer1_bottleneck1: 1456/4916\n",
      "Extracted activation of layer1_bottleneck1: 1464/4916\n",
      "Extracted activation of layer1_bottleneck1: 1472/4916\n",
      "Extracted activation of layer1_bottleneck1: 1480/4916\n",
      "Extracted activation of layer1_bottleneck1: 1488/4916\n",
      "Extracted activation of layer1_bottleneck1: 1496/4916\n",
      "Extracted activation of layer1_bottleneck1: 1504/4916\n",
      "Extracted activation of layer1_bottleneck1: 1512/4916\n",
      "Extracted activation of layer1_bottleneck1: 1520/4916\n",
      "Extracted activation of layer1_bottleneck1: 1528/4916\n",
      "Extracted activation of layer1_bottleneck1: 1536/4916\n",
      "Extracted activation of layer1_bottleneck1: 1544/4916\n",
      "Extracted activation of layer1_bottleneck1: 1552/4916\n",
      "Extracted activation of layer1_bottleneck1: 1560/4916\n",
      "Extracted activation of layer1_bottleneck1: 1568/4916\n",
      "Extracted activation of layer1_bottleneck1: 1576/4916\n",
      "Extracted activation of layer1_bottleneck1: 1584/4916\n",
      "Extracted activation of layer1_bottleneck1: 1592/4916\n",
      "Extracted activation of layer1_bottleneck1: 1600/4916\n",
      "Extracted activation of layer1_bottleneck1: 1608/4916\n",
      "Extracted activation of layer1_bottleneck1: 1616/4916\n",
      "Extracted activation of layer1_bottleneck1: 1624/4916\n",
      "Extracted activation of layer1_bottleneck1: 1632/4916\n",
      "Extracted activation of layer1_bottleneck1: 1640/4916\n",
      "Extracted activation of layer1_bottleneck1: 1648/4916\n",
      "Extracted activation of layer1_bottleneck1: 1656/4916\n",
      "Extracted activation of layer1_bottleneck1: 1664/4916\n",
      "Extracted activation of layer1_bottleneck1: 1672/4916\n",
      "Extracted activation of layer1_bottleneck1: 1680/4916\n",
      "Extracted activation of layer1_bottleneck1: 1688/4916\n",
      "Extracted activation of layer1_bottleneck1: 1696/4916\n",
      "Extracted activation of layer1_bottleneck1: 1704/4916\n",
      "Extracted activation of layer1_bottleneck1: 1712/4916\n",
      "Extracted activation of layer1_bottleneck1: 1720/4916\n",
      "Extracted activation of layer1_bottleneck1: 1728/4916\n",
      "Extracted activation of layer1_bottleneck1: 1736/4916\n",
      "Extracted activation of layer1_bottleneck1: 1744/4916\n",
      "Extracted activation of layer1_bottleneck1: 1752/4916\n",
      "Extracted activation of layer1_bottleneck1: 1760/4916\n",
      "Extracted activation of layer1_bottleneck1: 1768/4916\n",
      "Extracted activation of layer1_bottleneck1: 1776/4916\n",
      "Extracted activation of layer1_bottleneck1: 1784/4916\n",
      "Extracted activation of layer1_bottleneck1: 1792/4916\n",
      "Extracted activation of layer1_bottleneck1: 1800/4916\n",
      "Extracted activation of layer1_bottleneck1: 1808/4916\n",
      "Extracted activation of layer1_bottleneck1: 1816/4916\n",
      "Extracted activation of layer1_bottleneck1: 1824/4916\n",
      "Extracted activation of layer1_bottleneck1: 1832/4916\n",
      "Extracted activation of layer1_bottleneck1: 1840/4916\n",
      "Extracted activation of layer1_bottleneck1: 1848/4916\n",
      "Extracted activation of layer1_bottleneck1: 1856/4916\n",
      "Extracted activation of layer1_bottleneck1: 1864/4916\n",
      "Extracted activation of layer1_bottleneck1: 1872/4916\n",
      "Extracted activation of layer1_bottleneck1: 1880/4916\n",
      "Extracted activation of layer1_bottleneck1: 1888/4916\n",
      "Extracted activation of layer1_bottleneck1: 1896/4916\n",
      "Extracted activation of layer1_bottleneck1: 1904/4916\n",
      "Extracted activation of layer1_bottleneck1: 1912/4916\n",
      "Extracted activation of layer1_bottleneck1: 1920/4916\n",
      "Extracted activation of layer1_bottleneck1: 1928/4916\n",
      "Extracted activation of layer1_bottleneck1: 1936/4916\n",
      "Extracted activation of layer1_bottleneck1: 1944/4916\n",
      "Extracted activation of layer1_bottleneck1: 1952/4916\n",
      "Extracted activation of layer1_bottleneck1: 1960/4916\n",
      "Extracted activation of layer1_bottleneck1: 1968/4916\n",
      "Extracted activation of layer1_bottleneck1: 1976/4916\n",
      "Extracted activation of layer1_bottleneck1: 1984/4916\n",
      "Extracted activation of layer1_bottleneck1: 1992/4916\n",
      "Extracted activation of layer1_bottleneck1: 2000/4916\n",
      "Extracted activation of layer1_bottleneck1: 2008/4916\n",
      "Extracted activation of layer1_bottleneck1: 2016/4916\n",
      "Extracted activation of layer1_bottleneck1: 2024/4916\n",
      "Extracted activation of layer1_bottleneck1: 2032/4916\n",
      "Extracted activation of layer1_bottleneck1: 2040/4916\n",
      "Extracted activation of layer1_bottleneck1: 2048/4916\n",
      "Extracted activation of layer1_bottleneck1: 2056/4916\n",
      "Extracted activation of layer1_bottleneck1: 2064/4916\n",
      "Extracted activation of layer1_bottleneck1: 2072/4916\n",
      "Extracted activation of layer1_bottleneck1: 2080/4916\n",
      "Extracted activation of layer1_bottleneck1: 2088/4916\n",
      "Extracted activation of layer1_bottleneck1: 2096/4916\n",
      "Extracted activation of layer1_bottleneck1: 2104/4916\n",
      "Extracted activation of layer1_bottleneck1: 2112/4916\n",
      "Extracted activation of layer1_bottleneck1: 2120/4916\n",
      "Extracted activation of layer1_bottleneck1: 2128/4916\n",
      "Extracted activation of layer1_bottleneck1: 2136/4916\n",
      "Extracted activation of layer1_bottleneck1: 2144/4916\n",
      "Extracted activation of layer1_bottleneck1: 2152/4916\n",
      "Extracted activation of layer1_bottleneck1: 2160/4916\n",
      "Extracted activation of layer1_bottleneck1: 2168/4916\n",
      "Extracted activation of layer1_bottleneck1: 2176/4916\n",
      "Extracted activation of layer1_bottleneck1: 2184/4916\n",
      "Extracted activation of layer1_bottleneck1: 2192/4916\n",
      "Extracted activation of layer1_bottleneck1: 2200/4916\n",
      "Extracted activation of layer1_bottleneck1: 2208/4916\n",
      "Extracted activation of layer1_bottleneck1: 2216/4916\n",
      "Extracted activation of layer1_bottleneck1: 2224/4916\n",
      "Extracted activation of layer1_bottleneck1: 2232/4916\n",
      "Extracted activation of layer1_bottleneck1: 2240/4916\n",
      "Extracted activation of layer1_bottleneck1: 2248/4916\n",
      "Extracted activation of layer1_bottleneck1: 2256/4916\n",
      "Extracted activation of layer1_bottleneck1: 2264/4916\n",
      "Extracted activation of layer1_bottleneck1: 2272/4916\n",
      "Extracted activation of layer1_bottleneck1: 2280/4916\n",
      "Extracted activation of layer1_bottleneck1: 2288/4916\n",
      "Extracted activation of layer1_bottleneck1: 2296/4916\n",
      "Extracted activation of layer1_bottleneck1: 2304/4916\n",
      "Extracted activation of layer1_bottleneck1: 2312/4916\n",
      "Extracted activation of layer1_bottleneck1: 2320/4916\n",
      "Extracted activation of layer1_bottleneck1: 2328/4916\n",
      "Extracted activation of layer1_bottleneck1: 2336/4916\n",
      "Extracted activation of layer1_bottleneck1: 2344/4916\n",
      "Extracted activation of layer1_bottleneck1: 2352/4916\n",
      "Extracted activation of layer1_bottleneck1: 2360/4916\n",
      "Extracted activation of layer1_bottleneck1: 2368/4916\n",
      "Extracted activation of layer1_bottleneck1: 2376/4916\n",
      "Extracted activation of layer1_bottleneck1: 2384/4916\n",
      "Extracted activation of layer1_bottleneck1: 2392/4916\n",
      "Extracted activation of layer1_bottleneck1: 2400/4916\n",
      "Extracted activation of layer1_bottleneck1: 2408/4916\n",
      "Extracted activation of layer1_bottleneck1: 2416/4916\n",
      "Extracted activation of layer1_bottleneck1: 2424/4916\n",
      "Extracted activation of layer1_bottleneck1: 2432/4916\n",
      "Extracted activation of layer1_bottleneck1: 2440/4916\n",
      "Extracted activation of layer1_bottleneck1: 2448/4916\n",
      "Extracted activation of layer1_bottleneck1: 2456/4916\n",
      "Extracted activation of layer1_bottleneck1: 2464/4916\n",
      "Extracted activation of layer1_bottleneck1: 2472/4916\n",
      "Extracted activation of layer1_bottleneck1: 2480/4916\n",
      "Extracted activation of layer1_bottleneck1: 2488/4916\n",
      "Extracted activation of layer1_bottleneck1: 2496/4916\n",
      "Extracted activation of layer1_bottleneck1: 2504/4916\n",
      "Extracted activation of layer1_bottleneck1: 2512/4916\n",
      "Extracted activation of layer1_bottleneck1: 2520/4916\n",
      "Extracted activation of layer1_bottleneck1: 2528/4916\n",
      "Extracted activation of layer1_bottleneck1: 2536/4916\n",
      "Extracted activation of layer1_bottleneck1: 2544/4916\n",
      "Extracted activation of layer1_bottleneck1: 2552/4916\n",
      "Extracted activation of layer1_bottleneck1: 2560/4916\n",
      "Extracted activation of layer1_bottleneck1: 2568/4916\n",
      "Extracted activation of layer1_bottleneck1: 2576/4916\n",
      "Extracted activation of layer1_bottleneck1: 2584/4916\n",
      "Extracted activation of layer1_bottleneck1: 2592/4916\n",
      "Extracted activation of layer1_bottleneck1: 2600/4916\n",
      "Extracted activation of layer1_bottleneck1: 2608/4916\n",
      "Extracted activation of layer1_bottleneck1: 2616/4916\n",
      "Extracted activation of layer1_bottleneck1: 2624/4916\n",
      "Extracted activation of layer1_bottleneck1: 2632/4916\n",
      "Extracted activation of layer1_bottleneck1: 2640/4916\n",
      "Extracted activation of layer1_bottleneck1: 2648/4916\n",
      "Extracted activation of layer1_bottleneck1: 2656/4916\n",
      "Extracted activation of layer1_bottleneck1: 2664/4916\n",
      "Extracted activation of layer1_bottleneck1: 2672/4916\n",
      "Extracted activation of layer1_bottleneck1: 2680/4916\n",
      "Extracted activation of layer1_bottleneck1: 2688/4916\n",
      "Extracted activation of layer1_bottleneck1: 2696/4916\n",
      "Extracted activation of layer1_bottleneck1: 2704/4916\n",
      "Extracted activation of layer1_bottleneck1: 2712/4916\n",
      "Extracted activation of layer1_bottleneck1: 2720/4916\n",
      "Extracted activation of layer1_bottleneck1: 2728/4916\n",
      "Extracted activation of layer1_bottleneck1: 2736/4916\n",
      "Extracted activation of layer1_bottleneck1: 2744/4916\n",
      "Extracted activation of layer1_bottleneck1: 2752/4916\n",
      "Extracted activation of layer1_bottleneck1: 2760/4916\n",
      "Extracted activation of layer1_bottleneck1: 2768/4916\n",
      "Extracted activation of layer1_bottleneck1: 2776/4916\n",
      "Extracted activation of layer1_bottleneck1: 2784/4916\n",
      "Extracted activation of layer1_bottleneck1: 2792/4916\n",
      "Extracted activation of layer1_bottleneck1: 2800/4916\n",
      "Extracted activation of layer1_bottleneck1: 2808/4916\n",
      "Extracted activation of layer1_bottleneck1: 2816/4916\n",
      "Extracted activation of layer1_bottleneck1: 2824/4916\n",
      "Extracted activation of layer1_bottleneck1: 2832/4916\n",
      "Extracted activation of layer1_bottleneck1: 2840/4916\n",
      "Extracted activation of layer1_bottleneck1: 2848/4916\n",
      "Extracted activation of layer1_bottleneck1: 2856/4916\n",
      "Extracted activation of layer1_bottleneck1: 2864/4916\n",
      "Extracted activation of layer1_bottleneck1: 2872/4916\n",
      "Extracted activation of layer1_bottleneck1: 2880/4916\n",
      "Extracted activation of layer1_bottleneck1: 2888/4916\n",
      "Extracted activation of layer1_bottleneck1: 2896/4916\n",
      "Extracted activation of layer1_bottleneck1: 2904/4916\n",
      "Extracted activation of layer1_bottleneck1: 2912/4916\n",
      "Extracted activation of layer1_bottleneck1: 2920/4916\n",
      "Extracted activation of layer1_bottleneck1: 2928/4916\n",
      "Extracted activation of layer1_bottleneck1: 2936/4916\n",
      "Extracted activation of layer1_bottleneck1: 2944/4916\n",
      "Extracted activation of layer1_bottleneck1: 2952/4916\n",
      "Extracted activation of layer1_bottleneck1: 2960/4916\n",
      "Extracted activation of layer1_bottleneck1: 2968/4916\n",
      "Extracted activation of layer1_bottleneck1: 2976/4916\n",
      "Extracted activation of layer1_bottleneck1: 2984/4916\n",
      "Extracted activation of layer1_bottleneck1: 2992/4916\n",
      "Extracted activation of layer1_bottleneck1: 3000/4916\n",
      "Extracted activation of layer1_bottleneck1: 3008/4916\n",
      "Extracted activation of layer1_bottleneck1: 3016/4916\n",
      "Extracted activation of layer1_bottleneck1: 3024/4916\n",
      "Extracted activation of layer1_bottleneck1: 3032/4916\n",
      "Extracted activation of layer1_bottleneck1: 3040/4916\n",
      "Extracted activation of layer1_bottleneck1: 3048/4916\n",
      "Extracted activation of layer1_bottleneck1: 3056/4916\n",
      "Extracted activation of layer1_bottleneck1: 3064/4916\n",
      "Extracted activation of layer1_bottleneck1: 3072/4916\n",
      "Extracted activation of layer1_bottleneck1: 3080/4916\n",
      "Extracted activation of layer1_bottleneck1: 3088/4916\n",
      "Extracted activation of layer1_bottleneck1: 3096/4916\n",
      "Extracted activation of layer1_bottleneck1: 3104/4916\n",
      "Extracted activation of layer1_bottleneck1: 3112/4916\n",
      "Extracted activation of layer1_bottleneck1: 3120/4916\n",
      "Extracted activation of layer1_bottleneck1: 3128/4916\n",
      "Extracted activation of layer1_bottleneck1: 3136/4916\n",
      "Extracted activation of layer1_bottleneck1: 3144/4916\n",
      "Extracted activation of layer1_bottleneck1: 3152/4916\n",
      "Extracted activation of layer1_bottleneck1: 3160/4916\n",
      "Extracted activation of layer1_bottleneck1: 3168/4916\n",
      "Extracted activation of layer1_bottleneck1: 3176/4916\n",
      "Extracted activation of layer1_bottleneck1: 3184/4916\n",
      "Extracted activation of layer1_bottleneck1: 3192/4916\n",
      "Extracted activation of layer1_bottleneck1: 3200/4916\n",
      "Extracted activation of layer1_bottleneck1: 3208/4916\n",
      "Extracted activation of layer1_bottleneck1: 3216/4916\n",
      "Extracted activation of layer1_bottleneck1: 3224/4916\n",
      "Extracted activation of layer1_bottleneck1: 3232/4916\n",
      "Extracted activation of layer1_bottleneck1: 3240/4916\n",
      "Extracted activation of layer1_bottleneck1: 3248/4916\n",
      "Extracted activation of layer1_bottleneck1: 3256/4916\n",
      "Extracted activation of layer1_bottleneck1: 3264/4916\n",
      "Extracted activation of layer1_bottleneck1: 3272/4916\n",
      "Extracted activation of layer1_bottleneck1: 3280/4916\n",
      "Extracted activation of layer1_bottleneck1: 3288/4916\n",
      "Extracted activation of layer1_bottleneck1: 3296/4916\n",
      "Extracted activation of layer1_bottleneck1: 3304/4916\n",
      "Extracted activation of layer1_bottleneck1: 3312/4916\n",
      "Extracted activation of layer1_bottleneck1: 3320/4916\n",
      "Extracted activation of layer1_bottleneck1: 3328/4916\n",
      "Extracted activation of layer1_bottleneck1: 3336/4916\n",
      "Extracted activation of layer1_bottleneck1: 3344/4916\n",
      "Extracted activation of layer1_bottleneck1: 3352/4916\n",
      "Extracted activation of layer1_bottleneck1: 3360/4916\n",
      "Extracted activation of layer1_bottleneck1: 3368/4916\n",
      "Extracted activation of layer1_bottleneck1: 3376/4916\n",
      "Extracted activation of layer1_bottleneck1: 3384/4916\n",
      "Extracted activation of layer1_bottleneck1: 3392/4916\n",
      "Extracted activation of layer1_bottleneck1: 3400/4916\n",
      "Extracted activation of layer1_bottleneck1: 3408/4916\n",
      "Extracted activation of layer1_bottleneck1: 3416/4916\n",
      "Extracted activation of layer1_bottleneck1: 3424/4916\n",
      "Extracted activation of layer1_bottleneck1: 3432/4916\n",
      "Extracted activation of layer1_bottleneck1: 3440/4916\n",
      "Extracted activation of layer1_bottleneck1: 3448/4916\n",
      "Extracted activation of layer1_bottleneck1: 3456/4916\n",
      "Extracted activation of layer1_bottleneck1: 3464/4916\n",
      "Extracted activation of layer1_bottleneck1: 3472/4916\n",
      "Extracted activation of layer1_bottleneck1: 3480/4916\n",
      "Extracted activation of layer1_bottleneck1: 3488/4916\n",
      "Extracted activation of layer1_bottleneck1: 3496/4916\n",
      "Extracted activation of layer1_bottleneck1: 3504/4916\n",
      "Extracted activation of layer1_bottleneck1: 3512/4916\n",
      "Extracted activation of layer1_bottleneck1: 3520/4916\n",
      "Extracted activation of layer1_bottleneck1: 3528/4916\n",
      "Extracted activation of layer1_bottleneck1: 3536/4916\n",
      "Extracted activation of layer1_bottleneck1: 3544/4916\n",
      "Extracted activation of layer1_bottleneck1: 3552/4916\n",
      "Extracted activation of layer1_bottleneck1: 3560/4916\n",
      "Extracted activation of layer1_bottleneck1: 3568/4916\n",
      "Extracted activation of layer1_bottleneck1: 3576/4916\n",
      "Extracted activation of layer1_bottleneck1: 3584/4916\n",
      "Extracted activation of layer1_bottleneck1: 3592/4916\n",
      "Extracted activation of layer1_bottleneck1: 3600/4916\n",
      "Extracted activation of layer1_bottleneck1: 3608/4916\n",
      "Extracted activation of layer1_bottleneck1: 3616/4916\n",
      "Extracted activation of layer1_bottleneck1: 3624/4916\n",
      "Extracted activation of layer1_bottleneck1: 3632/4916\n",
      "Extracted activation of layer1_bottleneck1: 3640/4916\n",
      "Extracted activation of layer1_bottleneck1: 3648/4916\n",
      "Extracted activation of layer1_bottleneck1: 3656/4916\n",
      "Extracted activation of layer1_bottleneck1: 3664/4916\n",
      "Extracted activation of layer1_bottleneck1: 3672/4916\n",
      "Extracted activation of layer1_bottleneck1: 3680/4916\n",
      "Extracted activation of layer1_bottleneck1: 3688/4916\n",
      "Extracted activation of layer1_bottleneck1: 3696/4916\n",
      "Extracted activation of layer1_bottleneck1: 3704/4916\n",
      "Extracted activation of layer1_bottleneck1: 3712/4916\n",
      "Extracted activation of layer1_bottleneck1: 3720/4916\n",
      "Extracted activation of layer1_bottleneck1: 3728/4916\n",
      "Extracted activation of layer1_bottleneck1: 3736/4916\n",
      "Extracted activation of layer1_bottleneck1: 3744/4916\n",
      "Extracted activation of layer1_bottleneck1: 3752/4916\n",
      "Extracted activation of layer1_bottleneck1: 3760/4916\n",
      "Extracted activation of layer1_bottleneck1: 3768/4916\n",
      "Extracted activation of layer1_bottleneck1: 3776/4916\n",
      "Extracted activation of layer1_bottleneck1: 3784/4916\n",
      "Extracted activation of layer1_bottleneck1: 3792/4916\n",
      "Extracted activation of layer1_bottleneck1: 3800/4916\n",
      "Extracted activation of layer1_bottleneck1: 3808/4916\n",
      "Extracted activation of layer1_bottleneck1: 3816/4916\n",
      "Extracted activation of layer1_bottleneck1: 3824/4916\n",
      "Extracted activation of layer1_bottleneck1: 3832/4916\n",
      "Extracted activation of layer1_bottleneck1: 3840/4916\n",
      "Extracted activation of layer1_bottleneck1: 3848/4916\n",
      "Extracted activation of layer1_bottleneck1: 3856/4916\n",
      "Extracted activation of layer1_bottleneck1: 3864/4916\n",
      "Extracted activation of layer1_bottleneck1: 3872/4916\n",
      "Extracted activation of layer1_bottleneck1: 3880/4916\n",
      "Extracted activation of layer1_bottleneck1: 3888/4916\n",
      "Extracted activation of layer1_bottleneck1: 3896/4916\n",
      "Extracted activation of layer1_bottleneck1: 3904/4916\n",
      "Extracted activation of layer1_bottleneck1: 3912/4916\n",
      "Extracted activation of layer1_bottleneck1: 3920/4916\n",
      "Extracted activation of layer1_bottleneck1: 3928/4916\n",
      "Extracted activation of layer1_bottleneck1: 3936/4916\n",
      "Extracted activation of layer1_bottleneck1: 3944/4916\n",
      "Extracted activation of layer1_bottleneck1: 3952/4916\n",
      "Extracted activation of layer1_bottleneck1: 3960/4916\n",
      "Extracted activation of layer1_bottleneck1: 3968/4916\n",
      "Extracted activation of layer1_bottleneck1: 3976/4916\n",
      "Extracted activation of layer1_bottleneck1: 3984/4916\n",
      "Extracted activation of layer1_bottleneck1: 3992/4916\n",
      "Extracted activation of layer1_bottleneck1: 4000/4916\n",
      "Extracted activation of layer1_bottleneck1: 4008/4916\n",
      "Extracted activation of layer1_bottleneck1: 4016/4916\n",
      "Extracted activation of layer1_bottleneck1: 4024/4916\n",
      "Extracted activation of layer1_bottleneck1: 4032/4916\n",
      "Extracted activation of layer1_bottleneck1: 4040/4916\n",
      "Extracted activation of layer1_bottleneck1: 4048/4916\n",
      "Extracted activation of layer1_bottleneck1: 4056/4916\n",
      "Extracted activation of layer1_bottleneck1: 4064/4916\n",
      "Extracted activation of layer1_bottleneck1: 4072/4916\n",
      "Extracted activation of layer1_bottleneck1: 4080/4916\n",
      "Extracted activation of layer1_bottleneck1: 4088/4916\n",
      "Extracted activation of layer1_bottleneck1: 4096/4916\n",
      "Extracted activation of layer1_bottleneck1: 4104/4916\n",
      "Extracted activation of layer1_bottleneck1: 4112/4916\n",
      "Extracted activation of layer1_bottleneck1: 4120/4916\n",
      "Extracted activation of layer1_bottleneck1: 4128/4916\n",
      "Extracted activation of layer1_bottleneck1: 4136/4916\n",
      "Extracted activation of layer1_bottleneck1: 4144/4916\n",
      "Extracted activation of layer1_bottleneck1: 4152/4916\n",
      "Extracted activation of layer1_bottleneck1: 4160/4916\n",
      "Extracted activation of layer1_bottleneck1: 4168/4916\n",
      "Extracted activation of layer1_bottleneck1: 4176/4916\n",
      "Extracted activation of layer1_bottleneck1: 4184/4916\n",
      "Extracted activation of layer1_bottleneck1: 4192/4916\n",
      "Extracted activation of layer1_bottleneck1: 4200/4916\n",
      "Extracted activation of layer1_bottleneck1: 4208/4916\n",
      "Extracted activation of layer1_bottleneck1: 4216/4916\n",
      "Extracted activation of layer1_bottleneck1: 4224/4916\n",
      "Extracted activation of layer1_bottleneck1: 4232/4916\n",
      "Extracted activation of layer1_bottleneck1: 4240/4916\n",
      "Extracted activation of layer1_bottleneck1: 4248/4916\n",
      "Extracted activation of layer1_bottleneck1: 4256/4916\n",
      "Extracted activation of layer1_bottleneck1: 4264/4916\n",
      "Extracted activation of layer1_bottleneck1: 4272/4916\n",
      "Extracted activation of layer1_bottleneck1: 4280/4916\n",
      "Extracted activation of layer1_bottleneck1: 4288/4916\n",
      "Extracted activation of layer1_bottleneck1: 4296/4916\n",
      "Extracted activation of layer1_bottleneck1: 4304/4916\n",
      "Extracted activation of layer1_bottleneck1: 4312/4916\n",
      "Extracted activation of layer1_bottleneck1: 4320/4916\n",
      "Extracted activation of layer1_bottleneck1: 4328/4916\n",
      "Extracted activation of layer1_bottleneck1: 4336/4916\n",
      "Extracted activation of layer1_bottleneck1: 4344/4916\n",
      "Extracted activation of layer1_bottleneck1: 4352/4916\n",
      "Extracted activation of layer1_bottleneck1: 4360/4916\n",
      "Extracted activation of layer1_bottleneck1: 4368/4916\n",
      "Extracted activation of layer1_bottleneck1: 4376/4916\n",
      "Extracted activation of layer1_bottleneck1: 4384/4916\n",
      "Extracted activation of layer1_bottleneck1: 4392/4916\n",
      "Extracted activation of layer1_bottleneck1: 4400/4916\n",
      "Extracted activation of layer1_bottleneck1: 4408/4916\n",
      "Extracted activation of layer1_bottleneck1: 4416/4916\n",
      "Extracted activation of layer1_bottleneck1: 4424/4916\n",
      "Extracted activation of layer1_bottleneck1: 4432/4916\n",
      "Extracted activation of layer1_bottleneck1: 4440/4916\n",
      "Extracted activation of layer1_bottleneck1: 4448/4916\n",
      "Extracted activation of layer1_bottleneck1: 4456/4916\n",
      "Extracted activation of layer1_bottleneck1: 4464/4916\n",
      "Extracted activation of layer1_bottleneck1: 4472/4916\n",
      "Extracted activation of layer1_bottleneck1: 4480/4916\n",
      "Extracted activation of layer1_bottleneck1: 4488/4916\n",
      "Extracted activation of layer1_bottleneck1: 4496/4916\n",
      "Extracted activation of layer1_bottleneck1: 4504/4916\n",
      "Extracted activation of layer1_bottleneck1: 4512/4916\n",
      "Extracted activation of layer1_bottleneck1: 4520/4916\n",
      "Extracted activation of layer1_bottleneck1: 4528/4916\n",
      "Extracted activation of layer1_bottleneck1: 4536/4916\n",
      "Extracted activation of layer1_bottleneck1: 4544/4916\n",
      "Extracted activation of layer1_bottleneck1: 4552/4916\n",
      "Extracted activation of layer1_bottleneck1: 4560/4916\n",
      "Extracted activation of layer1_bottleneck1: 4568/4916\n",
      "Extracted activation of layer1_bottleneck1: 4576/4916\n",
      "Extracted activation of layer1_bottleneck1: 4584/4916\n",
      "Extracted activation of layer1_bottleneck1: 4592/4916\n",
      "Extracted activation of layer1_bottleneck1: 4600/4916\n",
      "Extracted activation of layer1_bottleneck1: 4608/4916\n",
      "Extracted activation of layer1_bottleneck1: 4616/4916\n",
      "Extracted activation of layer1_bottleneck1: 4624/4916\n",
      "Extracted activation of layer1_bottleneck1: 4632/4916\n",
      "Extracted activation of layer1_bottleneck1: 4640/4916\n",
      "Extracted activation of layer1_bottleneck1: 4648/4916\n",
      "Extracted activation of layer1_bottleneck1: 4656/4916\n",
      "Extracted activation of layer1_bottleneck1: 4664/4916\n",
      "Extracted activation of layer1_bottleneck1: 4672/4916\n",
      "Extracted activation of layer1_bottleneck1: 4680/4916\n",
      "Extracted activation of layer1_bottleneck1: 4688/4916\n",
      "Extracted activation of layer1_bottleneck1: 4696/4916\n",
      "Extracted activation of layer1_bottleneck1: 4704/4916\n",
      "Extracted activation of layer1_bottleneck1: 4712/4916\n",
      "Extracted activation of layer1_bottleneck1: 4720/4916\n",
      "Extracted activation of layer1_bottleneck1: 4728/4916\n",
      "Extracted activation of layer1_bottleneck1: 4736/4916\n",
      "Extracted activation of layer1_bottleneck1: 4744/4916\n",
      "Extracted activation of layer1_bottleneck1: 4752/4916\n",
      "Extracted activation of layer1_bottleneck1: 4760/4916\n",
      "Extracted activation of layer1_bottleneck1: 4768/4916\n",
      "Extracted activation of layer1_bottleneck1: 4776/4916\n",
      "Extracted activation of layer1_bottleneck1: 4784/4916\n",
      "Extracted activation of layer1_bottleneck1: 4792/4916\n",
      "Extracted activation of layer1_bottleneck1: 4800/4916\n",
      "Extracted activation of layer1_bottleneck1: 4808/4916\n",
      "Extracted activation of layer1_bottleneck1: 4816/4916\n",
      "Extracted activation of layer1_bottleneck1: 4824/4916\n",
      "Extracted activation of layer1_bottleneck1: 4832/4916\n",
      "Extracted activation of layer1_bottleneck1: 4840/4916\n",
      "Extracted activation of layer1_bottleneck1: 4848/4916\n",
      "Extracted activation of layer1_bottleneck1: 4856/4916\n",
      "Extracted activation of layer1_bottleneck1: 4864/4916\n",
      "Extracted activation of layer1_bottleneck1: 4872/4916\n",
      "Extracted activation of layer1_bottleneck1: 4880/4916\n",
      "Extracted activation of layer1_bottleneck1: 4888/4916\n",
      "Extracted activation of layer1_bottleneck1: 4896/4916\n",
      "Extracted activation of layer1_bottleneck1: 4904/4916\n",
      "Extracted activation of layer1_bottleneck1: 4912/4916\n",
      "Extracted activation of layer1_bottleneck1: 4916/4916\n",
      "Extracted activation of layer2_bottleneck1: 8/4916\n",
      "Extracted activation of layer2_bottleneck1: 16/4916\n",
      "Extracted activation of layer2_bottleneck1: 24/4916\n",
      "Extracted activation of layer2_bottleneck1: 32/4916\n",
      "Extracted activation of layer2_bottleneck1: 40/4916\n",
      "Extracted activation of layer2_bottleneck1: 48/4916\n",
      "Extracted activation of layer2_bottleneck1: 56/4916\n",
      "Extracted activation of layer2_bottleneck1: 64/4916\n",
      "Extracted activation of layer2_bottleneck1: 72/4916\n",
      "Extracted activation of layer2_bottleneck1: 80/4916\n",
      "Extracted activation of layer2_bottleneck1: 88/4916\n",
      "Extracted activation of layer2_bottleneck1: 96/4916\n",
      "Extracted activation of layer2_bottleneck1: 104/4916\n",
      "Extracted activation of layer2_bottleneck1: 112/4916\n",
      "Extracted activation of layer2_bottleneck1: 120/4916\n",
      "Extracted activation of layer2_bottleneck1: 128/4916\n",
      "Extracted activation of layer2_bottleneck1: 136/4916\n",
      "Extracted activation of layer2_bottleneck1: 144/4916\n",
      "Extracted activation of layer2_bottleneck1: 152/4916\n",
      "Extracted activation of layer2_bottleneck1: 160/4916\n",
      "Extracted activation of layer2_bottleneck1: 168/4916\n",
      "Extracted activation of layer2_bottleneck1: 176/4916\n",
      "Extracted activation of layer2_bottleneck1: 184/4916\n",
      "Extracted activation of layer2_bottleneck1: 192/4916\n",
      "Extracted activation of layer2_bottleneck1: 200/4916\n",
      "Extracted activation of layer2_bottleneck1: 208/4916\n",
      "Extracted activation of layer2_bottleneck1: 216/4916\n",
      "Extracted activation of layer2_bottleneck1: 224/4916\n",
      "Extracted activation of layer2_bottleneck1: 232/4916\n",
      "Extracted activation of layer2_bottleneck1: 240/4916\n",
      "Extracted activation of layer2_bottleneck1: 248/4916\n",
      "Extracted activation of layer2_bottleneck1: 256/4916\n",
      "Extracted activation of layer2_bottleneck1: 264/4916\n",
      "Extracted activation of layer2_bottleneck1: 272/4916\n",
      "Extracted activation of layer2_bottleneck1: 280/4916\n",
      "Extracted activation of layer2_bottleneck1: 288/4916\n",
      "Extracted activation of layer2_bottleneck1: 296/4916\n",
      "Extracted activation of layer2_bottleneck1: 304/4916\n",
      "Extracted activation of layer2_bottleneck1: 312/4916\n",
      "Extracted activation of layer2_bottleneck1: 320/4916\n",
      "Extracted activation of layer2_bottleneck1: 328/4916\n",
      "Extracted activation of layer2_bottleneck1: 336/4916\n",
      "Extracted activation of layer2_bottleneck1: 344/4916\n",
      "Extracted activation of layer2_bottleneck1: 352/4916\n",
      "Extracted activation of layer2_bottleneck1: 360/4916\n",
      "Extracted activation of layer2_bottleneck1: 368/4916\n",
      "Extracted activation of layer2_bottleneck1: 376/4916\n",
      "Extracted activation of layer2_bottleneck1: 384/4916\n",
      "Extracted activation of layer2_bottleneck1: 392/4916\n",
      "Extracted activation of layer2_bottleneck1: 400/4916\n",
      "Extracted activation of layer2_bottleneck1: 408/4916\n",
      "Extracted activation of layer2_bottleneck1: 416/4916\n",
      "Extracted activation of layer2_bottleneck1: 424/4916\n",
      "Extracted activation of layer2_bottleneck1: 432/4916\n",
      "Extracted activation of layer2_bottleneck1: 440/4916\n",
      "Extracted activation of layer2_bottleneck1: 448/4916\n",
      "Extracted activation of layer2_bottleneck1: 456/4916\n",
      "Extracted activation of layer2_bottleneck1: 464/4916\n",
      "Extracted activation of layer2_bottleneck1: 472/4916\n",
      "Extracted activation of layer2_bottleneck1: 480/4916\n",
      "Extracted activation of layer2_bottleneck1: 488/4916\n",
      "Extracted activation of layer2_bottleneck1: 496/4916\n",
      "Extracted activation of layer2_bottleneck1: 504/4916\n",
      "Extracted activation of layer2_bottleneck1: 512/4916\n",
      "Extracted activation of layer2_bottleneck1: 520/4916\n",
      "Extracted activation of layer2_bottleneck1: 528/4916\n",
      "Extracted activation of layer2_bottleneck1: 536/4916\n",
      "Extracted activation of layer2_bottleneck1: 544/4916\n",
      "Extracted activation of layer2_bottleneck1: 552/4916\n",
      "Extracted activation of layer2_bottleneck1: 560/4916\n",
      "Extracted activation of layer2_bottleneck1: 568/4916\n",
      "Extracted activation of layer2_bottleneck1: 576/4916\n",
      "Extracted activation of layer2_bottleneck1: 584/4916\n",
      "Extracted activation of layer2_bottleneck1: 592/4916\n",
      "Extracted activation of layer2_bottleneck1: 600/4916\n",
      "Extracted activation of layer2_bottleneck1: 608/4916\n",
      "Extracted activation of layer2_bottleneck1: 616/4916\n",
      "Extracted activation of layer2_bottleneck1: 624/4916\n",
      "Extracted activation of layer2_bottleneck1: 632/4916\n",
      "Extracted activation of layer2_bottleneck1: 640/4916\n",
      "Extracted activation of layer2_bottleneck1: 648/4916\n",
      "Extracted activation of layer2_bottleneck1: 656/4916\n",
      "Extracted activation of layer2_bottleneck1: 664/4916\n",
      "Extracted activation of layer2_bottleneck1: 672/4916\n",
      "Extracted activation of layer2_bottleneck1: 680/4916\n",
      "Extracted activation of layer2_bottleneck1: 688/4916\n",
      "Extracted activation of layer2_bottleneck1: 696/4916\n",
      "Extracted activation of layer2_bottleneck1: 704/4916\n",
      "Extracted activation of layer2_bottleneck1: 712/4916\n",
      "Extracted activation of layer2_bottleneck1: 720/4916\n",
      "Extracted activation of layer2_bottleneck1: 728/4916\n",
      "Extracted activation of layer2_bottleneck1: 736/4916\n",
      "Extracted activation of layer2_bottleneck1: 744/4916\n",
      "Extracted activation of layer2_bottleneck1: 752/4916\n",
      "Extracted activation of layer2_bottleneck1: 760/4916\n",
      "Extracted activation of layer2_bottleneck1: 768/4916\n",
      "Extracted activation of layer2_bottleneck1: 776/4916\n",
      "Extracted activation of layer2_bottleneck1: 784/4916\n",
      "Extracted activation of layer2_bottleneck1: 792/4916\n",
      "Extracted activation of layer2_bottleneck1: 800/4916\n",
      "Extracted activation of layer2_bottleneck1: 808/4916\n",
      "Extracted activation of layer2_bottleneck1: 816/4916\n",
      "Extracted activation of layer2_bottleneck1: 824/4916\n",
      "Extracted activation of layer2_bottleneck1: 832/4916\n",
      "Extracted activation of layer2_bottleneck1: 840/4916\n",
      "Extracted activation of layer2_bottleneck1: 848/4916\n",
      "Extracted activation of layer2_bottleneck1: 856/4916\n",
      "Extracted activation of layer2_bottleneck1: 864/4916\n",
      "Extracted activation of layer2_bottleneck1: 872/4916\n",
      "Extracted activation of layer2_bottleneck1: 880/4916\n",
      "Extracted activation of layer2_bottleneck1: 888/4916\n",
      "Extracted activation of layer2_bottleneck1: 896/4916\n",
      "Extracted activation of layer2_bottleneck1: 904/4916\n",
      "Extracted activation of layer2_bottleneck1: 912/4916\n",
      "Extracted activation of layer2_bottleneck1: 920/4916\n",
      "Extracted activation of layer2_bottleneck1: 928/4916\n",
      "Extracted activation of layer2_bottleneck1: 936/4916\n",
      "Extracted activation of layer2_bottleneck1: 944/4916\n",
      "Extracted activation of layer2_bottleneck1: 952/4916\n",
      "Extracted activation of layer2_bottleneck1: 960/4916\n",
      "Extracted activation of layer2_bottleneck1: 968/4916\n",
      "Extracted activation of layer2_bottleneck1: 976/4916\n",
      "Extracted activation of layer2_bottleneck1: 984/4916\n",
      "Extracted activation of layer2_bottleneck1: 992/4916\n",
      "Extracted activation of layer2_bottleneck1: 1000/4916\n",
      "Extracted activation of layer2_bottleneck1: 1008/4916\n",
      "Extracted activation of layer2_bottleneck1: 1016/4916\n",
      "Extracted activation of layer2_bottleneck1: 1024/4916\n",
      "Extracted activation of layer2_bottleneck1: 1032/4916\n",
      "Extracted activation of layer2_bottleneck1: 1040/4916\n",
      "Extracted activation of layer2_bottleneck1: 1048/4916\n",
      "Extracted activation of layer2_bottleneck1: 1056/4916\n",
      "Extracted activation of layer2_bottleneck1: 1064/4916\n",
      "Extracted activation of layer2_bottleneck1: 1072/4916\n",
      "Extracted activation of layer2_bottleneck1: 1080/4916\n",
      "Extracted activation of layer2_bottleneck1: 1088/4916\n",
      "Extracted activation of layer2_bottleneck1: 1096/4916\n",
      "Extracted activation of layer2_bottleneck1: 1104/4916\n",
      "Extracted activation of layer2_bottleneck1: 1112/4916\n",
      "Extracted activation of layer2_bottleneck1: 1120/4916\n",
      "Extracted activation of layer2_bottleneck1: 1128/4916\n",
      "Extracted activation of layer2_bottleneck1: 1136/4916\n",
      "Extracted activation of layer2_bottleneck1: 1144/4916\n",
      "Extracted activation of layer2_bottleneck1: 1152/4916\n",
      "Extracted activation of layer2_bottleneck1: 1160/4916\n",
      "Extracted activation of layer2_bottleneck1: 1168/4916\n",
      "Extracted activation of layer2_bottleneck1: 1176/4916\n",
      "Extracted activation of layer2_bottleneck1: 1184/4916\n",
      "Extracted activation of layer2_bottleneck1: 1192/4916\n",
      "Extracted activation of layer2_bottleneck1: 1200/4916\n",
      "Extracted activation of layer2_bottleneck1: 1208/4916\n",
      "Extracted activation of layer2_bottleneck1: 1216/4916\n",
      "Extracted activation of layer2_bottleneck1: 1224/4916\n",
      "Extracted activation of layer2_bottleneck1: 1232/4916\n",
      "Extracted activation of layer2_bottleneck1: 1240/4916\n",
      "Extracted activation of layer2_bottleneck1: 1248/4916\n",
      "Extracted activation of layer2_bottleneck1: 1256/4916\n",
      "Extracted activation of layer2_bottleneck1: 1264/4916\n",
      "Extracted activation of layer2_bottleneck1: 1272/4916\n",
      "Extracted activation of layer2_bottleneck1: 1280/4916\n",
      "Extracted activation of layer2_bottleneck1: 1288/4916\n",
      "Extracted activation of layer2_bottleneck1: 1296/4916\n",
      "Extracted activation of layer2_bottleneck1: 1304/4916\n",
      "Extracted activation of layer2_bottleneck1: 1312/4916\n",
      "Extracted activation of layer2_bottleneck1: 1320/4916\n",
      "Extracted activation of layer2_bottleneck1: 1328/4916\n",
      "Extracted activation of layer2_bottleneck1: 1336/4916\n",
      "Extracted activation of layer2_bottleneck1: 1344/4916\n",
      "Extracted activation of layer2_bottleneck1: 1352/4916\n",
      "Extracted activation of layer2_bottleneck1: 1360/4916\n",
      "Extracted activation of layer2_bottleneck1: 1368/4916\n",
      "Extracted activation of layer2_bottleneck1: 1376/4916\n",
      "Extracted activation of layer2_bottleneck1: 1384/4916\n",
      "Extracted activation of layer2_bottleneck1: 1392/4916\n",
      "Extracted activation of layer2_bottleneck1: 1400/4916\n",
      "Extracted activation of layer2_bottleneck1: 1408/4916\n",
      "Extracted activation of layer2_bottleneck1: 1416/4916\n",
      "Extracted activation of layer2_bottleneck1: 1424/4916\n",
      "Extracted activation of layer2_bottleneck1: 1432/4916\n",
      "Extracted activation of layer2_bottleneck1: 1440/4916\n",
      "Extracted activation of layer2_bottleneck1: 1448/4916\n",
      "Extracted activation of layer2_bottleneck1: 1456/4916\n",
      "Extracted activation of layer2_bottleneck1: 1464/4916\n",
      "Extracted activation of layer2_bottleneck1: 1472/4916\n",
      "Extracted activation of layer2_bottleneck1: 1480/4916\n",
      "Extracted activation of layer2_bottleneck1: 1488/4916\n",
      "Extracted activation of layer2_bottleneck1: 1496/4916\n",
      "Extracted activation of layer2_bottleneck1: 1504/4916\n",
      "Extracted activation of layer2_bottleneck1: 1512/4916\n",
      "Extracted activation of layer2_bottleneck1: 1520/4916\n",
      "Extracted activation of layer2_bottleneck1: 1528/4916\n",
      "Extracted activation of layer2_bottleneck1: 1536/4916\n",
      "Extracted activation of layer2_bottleneck1: 1544/4916\n",
      "Extracted activation of layer2_bottleneck1: 1552/4916\n",
      "Extracted activation of layer2_bottleneck1: 1560/4916\n",
      "Extracted activation of layer2_bottleneck1: 1568/4916\n",
      "Extracted activation of layer2_bottleneck1: 1576/4916\n",
      "Extracted activation of layer2_bottleneck1: 1584/4916\n",
      "Extracted activation of layer2_bottleneck1: 1592/4916\n",
      "Extracted activation of layer2_bottleneck1: 1600/4916\n",
      "Extracted activation of layer2_bottleneck1: 1608/4916\n",
      "Extracted activation of layer2_bottleneck1: 1616/4916\n",
      "Extracted activation of layer2_bottleneck1: 1624/4916\n",
      "Extracted activation of layer2_bottleneck1: 1632/4916\n",
      "Extracted activation of layer2_bottleneck1: 1640/4916\n",
      "Extracted activation of layer2_bottleneck1: 1648/4916\n",
      "Extracted activation of layer2_bottleneck1: 1656/4916\n",
      "Extracted activation of layer2_bottleneck1: 1664/4916\n",
      "Extracted activation of layer2_bottleneck1: 1672/4916\n",
      "Extracted activation of layer2_bottleneck1: 1680/4916\n",
      "Extracted activation of layer2_bottleneck1: 1688/4916\n",
      "Extracted activation of layer2_bottleneck1: 1696/4916\n",
      "Extracted activation of layer2_bottleneck1: 1704/4916\n",
      "Extracted activation of layer2_bottleneck1: 1712/4916\n",
      "Extracted activation of layer2_bottleneck1: 1720/4916\n",
      "Extracted activation of layer2_bottleneck1: 1728/4916\n",
      "Extracted activation of layer2_bottleneck1: 1736/4916\n",
      "Extracted activation of layer2_bottleneck1: 1744/4916\n",
      "Extracted activation of layer2_bottleneck1: 1752/4916\n",
      "Extracted activation of layer2_bottleneck1: 1760/4916\n",
      "Extracted activation of layer2_bottleneck1: 1768/4916\n",
      "Extracted activation of layer2_bottleneck1: 1776/4916\n",
      "Extracted activation of layer2_bottleneck1: 1784/4916\n",
      "Extracted activation of layer2_bottleneck1: 1792/4916\n",
      "Extracted activation of layer2_bottleneck1: 1800/4916\n",
      "Extracted activation of layer2_bottleneck1: 1808/4916\n",
      "Extracted activation of layer2_bottleneck1: 1816/4916\n",
      "Extracted activation of layer2_bottleneck1: 1824/4916\n",
      "Extracted activation of layer2_bottleneck1: 1832/4916\n",
      "Extracted activation of layer2_bottleneck1: 1840/4916\n",
      "Extracted activation of layer2_bottleneck1: 1848/4916\n",
      "Extracted activation of layer2_bottleneck1: 1856/4916\n",
      "Extracted activation of layer2_bottleneck1: 1864/4916\n",
      "Extracted activation of layer2_bottleneck1: 1872/4916\n",
      "Extracted activation of layer2_bottleneck1: 1880/4916\n",
      "Extracted activation of layer2_bottleneck1: 1888/4916\n",
      "Extracted activation of layer2_bottleneck1: 1896/4916\n",
      "Extracted activation of layer2_bottleneck1: 1904/4916\n",
      "Extracted activation of layer2_bottleneck1: 1912/4916\n",
      "Extracted activation of layer2_bottleneck1: 1920/4916\n",
      "Extracted activation of layer2_bottleneck1: 1928/4916\n",
      "Extracted activation of layer2_bottleneck1: 1936/4916\n",
      "Extracted activation of layer2_bottleneck1: 1944/4916\n",
      "Extracted activation of layer2_bottleneck1: 1952/4916\n",
      "Extracted activation of layer2_bottleneck1: 1960/4916\n",
      "Extracted activation of layer2_bottleneck1: 1968/4916\n",
      "Extracted activation of layer2_bottleneck1: 1976/4916\n",
      "Extracted activation of layer2_bottleneck1: 1984/4916\n",
      "Extracted activation of layer2_bottleneck1: 1992/4916\n",
      "Extracted activation of layer2_bottleneck1: 2000/4916\n",
      "Extracted activation of layer2_bottleneck1: 2008/4916\n",
      "Extracted activation of layer2_bottleneck1: 2016/4916\n",
      "Extracted activation of layer2_bottleneck1: 2024/4916\n",
      "Extracted activation of layer2_bottleneck1: 2032/4916\n",
      "Extracted activation of layer2_bottleneck1: 2040/4916\n",
      "Extracted activation of layer2_bottleneck1: 2048/4916\n",
      "Extracted activation of layer2_bottleneck1: 2056/4916\n",
      "Extracted activation of layer2_bottleneck1: 2064/4916\n",
      "Extracted activation of layer2_bottleneck1: 2072/4916\n",
      "Extracted activation of layer2_bottleneck1: 2080/4916\n",
      "Extracted activation of layer2_bottleneck1: 2088/4916\n",
      "Extracted activation of layer2_bottleneck1: 2096/4916\n",
      "Extracted activation of layer2_bottleneck1: 2104/4916\n",
      "Extracted activation of layer2_bottleneck1: 2112/4916\n",
      "Extracted activation of layer2_bottleneck1: 2120/4916\n",
      "Extracted activation of layer2_bottleneck1: 2128/4916\n",
      "Extracted activation of layer2_bottleneck1: 2136/4916\n",
      "Extracted activation of layer2_bottleneck1: 2144/4916\n",
      "Extracted activation of layer2_bottleneck1: 2152/4916\n",
      "Extracted activation of layer2_bottleneck1: 2160/4916\n",
      "Extracted activation of layer2_bottleneck1: 2168/4916\n",
      "Extracted activation of layer2_bottleneck1: 2176/4916\n",
      "Extracted activation of layer2_bottleneck1: 2184/4916\n",
      "Extracted activation of layer2_bottleneck1: 2192/4916\n",
      "Extracted activation of layer2_bottleneck1: 2200/4916\n",
      "Extracted activation of layer2_bottleneck1: 2208/4916\n",
      "Extracted activation of layer2_bottleneck1: 2216/4916\n",
      "Extracted activation of layer2_bottleneck1: 2224/4916\n",
      "Extracted activation of layer2_bottleneck1: 2232/4916\n",
      "Extracted activation of layer2_bottleneck1: 2240/4916\n",
      "Extracted activation of layer2_bottleneck1: 2248/4916\n",
      "Extracted activation of layer2_bottleneck1: 2256/4916\n",
      "Extracted activation of layer2_bottleneck1: 2264/4916\n",
      "Extracted activation of layer2_bottleneck1: 2272/4916\n",
      "Extracted activation of layer2_bottleneck1: 2280/4916\n",
      "Extracted activation of layer2_bottleneck1: 2288/4916\n",
      "Extracted activation of layer2_bottleneck1: 2296/4916\n",
      "Extracted activation of layer2_bottleneck1: 2304/4916\n",
      "Extracted activation of layer2_bottleneck1: 2312/4916\n",
      "Extracted activation of layer2_bottleneck1: 2320/4916\n",
      "Extracted activation of layer2_bottleneck1: 2328/4916\n",
      "Extracted activation of layer2_bottleneck1: 2336/4916\n",
      "Extracted activation of layer2_bottleneck1: 2344/4916\n",
      "Extracted activation of layer2_bottleneck1: 2352/4916\n",
      "Extracted activation of layer2_bottleneck1: 2360/4916\n",
      "Extracted activation of layer2_bottleneck1: 2368/4916\n",
      "Extracted activation of layer2_bottleneck1: 2376/4916\n",
      "Extracted activation of layer2_bottleneck1: 2384/4916\n",
      "Extracted activation of layer2_bottleneck1: 2392/4916\n",
      "Extracted activation of layer2_bottleneck1: 2400/4916\n",
      "Extracted activation of layer2_bottleneck1: 2408/4916\n",
      "Extracted activation of layer2_bottleneck1: 2416/4916\n",
      "Extracted activation of layer2_bottleneck1: 2424/4916\n",
      "Extracted activation of layer2_bottleneck1: 2432/4916\n",
      "Extracted activation of layer2_bottleneck1: 2440/4916\n",
      "Extracted activation of layer2_bottleneck1: 2448/4916\n",
      "Extracted activation of layer2_bottleneck1: 2456/4916\n",
      "Extracted activation of layer2_bottleneck1: 2464/4916\n",
      "Extracted activation of layer2_bottleneck1: 2472/4916\n",
      "Extracted activation of layer2_bottleneck1: 2480/4916\n",
      "Extracted activation of layer2_bottleneck1: 2488/4916\n",
      "Extracted activation of layer2_bottleneck1: 2496/4916\n",
      "Extracted activation of layer2_bottleneck1: 2504/4916\n",
      "Extracted activation of layer2_bottleneck1: 2512/4916\n",
      "Extracted activation of layer2_bottleneck1: 2520/4916\n",
      "Extracted activation of layer2_bottleneck1: 2528/4916\n",
      "Extracted activation of layer2_bottleneck1: 2536/4916\n",
      "Extracted activation of layer2_bottleneck1: 2544/4916\n",
      "Extracted activation of layer2_bottleneck1: 2552/4916\n",
      "Extracted activation of layer2_bottleneck1: 2560/4916\n",
      "Extracted activation of layer2_bottleneck1: 2568/4916\n",
      "Extracted activation of layer2_bottleneck1: 2576/4916\n",
      "Extracted activation of layer2_bottleneck1: 2584/4916\n",
      "Extracted activation of layer2_bottleneck1: 2592/4916\n",
      "Extracted activation of layer2_bottleneck1: 2600/4916\n",
      "Extracted activation of layer2_bottleneck1: 2608/4916\n",
      "Extracted activation of layer2_bottleneck1: 2616/4916\n",
      "Extracted activation of layer2_bottleneck1: 2624/4916\n",
      "Extracted activation of layer2_bottleneck1: 2632/4916\n",
      "Extracted activation of layer2_bottleneck1: 2640/4916\n",
      "Extracted activation of layer2_bottleneck1: 2648/4916\n",
      "Extracted activation of layer2_bottleneck1: 2656/4916\n",
      "Extracted activation of layer2_bottleneck1: 2664/4916\n",
      "Extracted activation of layer2_bottleneck1: 2672/4916\n",
      "Extracted activation of layer2_bottleneck1: 2680/4916\n",
      "Extracted activation of layer2_bottleneck1: 2688/4916\n",
      "Extracted activation of layer2_bottleneck1: 2696/4916\n",
      "Extracted activation of layer2_bottleneck1: 2704/4916\n",
      "Extracted activation of layer2_bottleneck1: 2712/4916\n",
      "Extracted activation of layer2_bottleneck1: 2720/4916\n",
      "Extracted activation of layer2_bottleneck1: 2728/4916\n",
      "Extracted activation of layer2_bottleneck1: 2736/4916\n",
      "Extracted activation of layer2_bottleneck1: 2744/4916\n",
      "Extracted activation of layer2_bottleneck1: 2752/4916\n",
      "Extracted activation of layer2_bottleneck1: 2760/4916\n",
      "Extracted activation of layer2_bottleneck1: 2768/4916\n",
      "Extracted activation of layer2_bottleneck1: 2776/4916\n",
      "Extracted activation of layer2_bottleneck1: 2784/4916\n",
      "Extracted activation of layer2_bottleneck1: 2792/4916\n",
      "Extracted activation of layer2_bottleneck1: 2800/4916\n",
      "Extracted activation of layer2_bottleneck1: 2808/4916\n",
      "Extracted activation of layer2_bottleneck1: 2816/4916\n",
      "Extracted activation of layer2_bottleneck1: 2824/4916\n",
      "Extracted activation of layer2_bottleneck1: 2832/4916\n",
      "Extracted activation of layer2_bottleneck1: 2840/4916\n",
      "Extracted activation of layer2_bottleneck1: 2848/4916\n",
      "Extracted activation of layer2_bottleneck1: 2856/4916\n",
      "Extracted activation of layer2_bottleneck1: 2864/4916\n",
      "Extracted activation of layer2_bottleneck1: 2872/4916\n",
      "Extracted activation of layer2_bottleneck1: 2880/4916\n",
      "Extracted activation of layer2_bottleneck1: 2888/4916\n",
      "Extracted activation of layer2_bottleneck1: 2896/4916\n",
      "Extracted activation of layer2_bottleneck1: 2904/4916\n",
      "Extracted activation of layer2_bottleneck1: 2912/4916\n",
      "Extracted activation of layer2_bottleneck1: 2920/4916\n",
      "Extracted activation of layer2_bottleneck1: 2928/4916\n",
      "Extracted activation of layer2_bottleneck1: 2936/4916\n",
      "Extracted activation of layer2_bottleneck1: 2944/4916\n",
      "Extracted activation of layer2_bottleneck1: 2952/4916\n",
      "Extracted activation of layer2_bottleneck1: 2960/4916\n",
      "Extracted activation of layer2_bottleneck1: 2968/4916\n",
      "Extracted activation of layer2_bottleneck1: 2976/4916\n",
      "Extracted activation of layer2_bottleneck1: 2984/4916\n",
      "Extracted activation of layer2_bottleneck1: 2992/4916\n",
      "Extracted activation of layer2_bottleneck1: 3000/4916\n",
      "Extracted activation of layer2_bottleneck1: 3008/4916\n",
      "Extracted activation of layer2_bottleneck1: 3016/4916\n",
      "Extracted activation of layer2_bottleneck1: 3024/4916\n",
      "Extracted activation of layer2_bottleneck1: 3032/4916\n",
      "Extracted activation of layer2_bottleneck1: 3040/4916\n",
      "Extracted activation of layer2_bottleneck1: 3048/4916\n",
      "Extracted activation of layer2_bottleneck1: 3056/4916\n",
      "Extracted activation of layer2_bottleneck1: 3064/4916\n",
      "Extracted activation of layer2_bottleneck1: 3072/4916\n",
      "Extracted activation of layer2_bottleneck1: 3080/4916\n",
      "Extracted activation of layer2_bottleneck1: 3088/4916\n",
      "Extracted activation of layer2_bottleneck1: 3096/4916\n",
      "Extracted activation of layer2_bottleneck1: 3104/4916\n",
      "Extracted activation of layer2_bottleneck1: 3112/4916\n",
      "Extracted activation of layer2_bottleneck1: 3120/4916\n",
      "Extracted activation of layer2_bottleneck1: 3128/4916\n",
      "Extracted activation of layer2_bottleneck1: 3136/4916\n",
      "Extracted activation of layer2_bottleneck1: 3144/4916\n",
      "Extracted activation of layer2_bottleneck1: 3152/4916\n",
      "Extracted activation of layer2_bottleneck1: 3160/4916\n",
      "Extracted activation of layer2_bottleneck1: 3168/4916\n",
      "Extracted activation of layer2_bottleneck1: 3176/4916\n",
      "Extracted activation of layer2_bottleneck1: 3184/4916\n",
      "Extracted activation of layer2_bottleneck1: 3192/4916\n",
      "Extracted activation of layer2_bottleneck1: 3200/4916\n",
      "Extracted activation of layer2_bottleneck1: 3208/4916\n",
      "Extracted activation of layer2_bottleneck1: 3216/4916\n",
      "Extracted activation of layer2_bottleneck1: 3224/4916\n",
      "Extracted activation of layer2_bottleneck1: 3232/4916\n",
      "Extracted activation of layer2_bottleneck1: 3240/4916\n",
      "Extracted activation of layer2_bottleneck1: 3248/4916\n",
      "Extracted activation of layer2_bottleneck1: 3256/4916\n",
      "Extracted activation of layer2_bottleneck1: 3264/4916\n",
      "Extracted activation of layer2_bottleneck1: 3272/4916\n",
      "Extracted activation of layer2_bottleneck1: 3280/4916\n",
      "Extracted activation of layer2_bottleneck1: 3288/4916\n",
      "Extracted activation of layer2_bottleneck1: 3296/4916\n",
      "Extracted activation of layer2_bottleneck1: 3304/4916\n",
      "Extracted activation of layer2_bottleneck1: 3312/4916\n",
      "Extracted activation of layer2_bottleneck1: 3320/4916\n",
      "Extracted activation of layer2_bottleneck1: 3328/4916\n",
      "Extracted activation of layer2_bottleneck1: 3336/4916\n",
      "Extracted activation of layer2_bottleneck1: 3344/4916\n",
      "Extracted activation of layer2_bottleneck1: 3352/4916\n",
      "Extracted activation of layer2_bottleneck1: 3360/4916\n",
      "Extracted activation of layer2_bottleneck1: 3368/4916\n",
      "Extracted activation of layer2_bottleneck1: 3376/4916\n",
      "Extracted activation of layer2_bottleneck1: 3384/4916\n",
      "Extracted activation of layer2_bottleneck1: 3392/4916\n",
      "Extracted activation of layer2_bottleneck1: 3400/4916\n",
      "Extracted activation of layer2_bottleneck1: 3408/4916\n",
      "Extracted activation of layer2_bottleneck1: 3416/4916\n",
      "Extracted activation of layer2_bottleneck1: 3424/4916\n",
      "Extracted activation of layer2_bottleneck1: 3432/4916\n",
      "Extracted activation of layer2_bottleneck1: 3440/4916\n",
      "Extracted activation of layer2_bottleneck1: 3448/4916\n",
      "Extracted activation of layer2_bottleneck1: 3456/4916\n",
      "Extracted activation of layer2_bottleneck1: 3464/4916\n",
      "Extracted activation of layer2_bottleneck1: 3472/4916\n",
      "Extracted activation of layer2_bottleneck1: 3480/4916\n",
      "Extracted activation of layer2_bottleneck1: 3488/4916\n",
      "Extracted activation of layer2_bottleneck1: 3496/4916\n",
      "Extracted activation of layer2_bottleneck1: 3504/4916\n",
      "Extracted activation of layer2_bottleneck1: 3512/4916\n",
      "Extracted activation of layer2_bottleneck1: 3520/4916\n",
      "Extracted activation of layer2_bottleneck1: 3528/4916\n",
      "Extracted activation of layer2_bottleneck1: 3536/4916\n",
      "Extracted activation of layer2_bottleneck1: 3544/4916\n",
      "Extracted activation of layer2_bottleneck1: 3552/4916\n",
      "Extracted activation of layer2_bottleneck1: 3560/4916\n",
      "Extracted activation of layer2_bottleneck1: 3568/4916\n",
      "Extracted activation of layer2_bottleneck1: 3576/4916\n",
      "Extracted activation of layer2_bottleneck1: 3584/4916\n",
      "Extracted activation of layer2_bottleneck1: 3592/4916\n",
      "Extracted activation of layer2_bottleneck1: 3600/4916\n",
      "Extracted activation of layer2_bottleneck1: 3608/4916\n",
      "Extracted activation of layer2_bottleneck1: 3616/4916\n",
      "Extracted activation of layer2_bottleneck1: 3624/4916\n",
      "Extracted activation of layer2_bottleneck1: 3632/4916\n",
      "Extracted activation of layer2_bottleneck1: 3640/4916\n",
      "Extracted activation of layer2_bottleneck1: 3648/4916\n",
      "Extracted activation of layer2_bottleneck1: 3656/4916\n",
      "Extracted activation of layer2_bottleneck1: 3664/4916\n",
      "Extracted activation of layer2_bottleneck1: 3672/4916\n",
      "Extracted activation of layer2_bottleneck1: 3680/4916\n",
      "Extracted activation of layer2_bottleneck1: 3688/4916\n"
     ]
    }
   ],
   "source": [
    "activation = dnn.compute_activation(stimuli, dmask)\n",
    "# activation.save(out_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-01T18:32:58.922684Z"
    }
   },
   "id": "c94082bab3257c0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "for layer in activation.layers:\n",
    "    activ_arr = activation.get(layer)\n",
    "    shape = activ_arr.shape\n",
    "    activ_arr = activ_arr.reshape((shape[0], -1))\n",
    "    activ_arr = zscore(activ_arr, axis=1)\n",
    "    activation.set(layer, activ_arr.reshape(shape))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-01T14:12:02.098256Z"
    }
   },
   "id": "4af120fc82aeca69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dnn_fe -act AlexNet_relu_zscore.act.h5 -meth pca 100 -out AlexNet_relu_zscore_PCA-100.act.h5\n",
    "\n",
    "activation = activation.fe(meth, n_feats, None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63b049aa25f0a7ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activation.save(out_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea8ef8afab712b13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dnn_rsa -act AlexNet_relu_zscore_PCA-100.act.h5 -metric correlation -out AlexNet_relu_zscore_PCA-100.rdm.h5\n",
    "\n",
    "from dnnbrain_lib import RDM\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "rdm = RDM()\n",
    "rdm.rdm_type = 'dRDM'\n",
    "for layer in activation.layers:\n",
    "    time1 = time.time()\n",
    "    # get DNN activation and reshape it to 3D\n",
    "    activ = activation.get(layer)\n",
    "    n_stim, n_chn, n_row, n_col = activ.shape\n",
    "    n_row_col = n_row * n_col\n",
    "    activ = activ.reshape((n_stim, n_chn, n_row_col))\n",
    "\n",
    "    # transpose axis to make activ's shape as (n_stimulus, n_iterator, n_element)\n",
    "    activ = activ.reshape((n_stim, 1, -1))\n",
    "\n",
    "    n_stim, n_iter, n_elem = activ.shape\n",
    "\n",
    "    # calculate RDM\n",
    "    rdm_arr = []\n",
    "    for iter_idx in range(n_iter):\n",
    "        X = activ[:, iter_idx, :]\n",
    "        rdm_arr.append(pdist(X, metric=\"correlation\"))\n",
    "    rdm.set(layer, np.asarray(rdm_arr), triu=True)\n",
    "    print('Finished layer-{}: cost {} seconds'.format(layer, time.time() - time1))\n",
    "\n",
    "# save\n",
    "rdm.save(rdm_out)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45ceb33f2fde8fae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eef91e5ab62ec699"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
